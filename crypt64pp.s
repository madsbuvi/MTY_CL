# 1 "crypt64.S"
# 1 "<command-line>"
# 1 "crypt64.S"
# 117 "crypt64.S"
.if 0
# 1 "x86-sse.S" 1
# 42 "x86-sse.S"
.bss


.globl DES_bs_all
.align (1 << (5))
DES_bs_all:
DES_bs_all_KSp:
.space (0x300 * 4)
DES_bs_all_KS_p:
DES_bs_all_KS_v:
.space (0x300 * 16)
DES_bs_all_E:
.space (96 * 4)
DES_bs_all_K:
.space (56 * 16)
DES_bs_all_B:
.space (64 * 16)
DES_bs_all_tmp:
.space (16 * 16)
DES_bs_all_fields_not_used_here:
.space (0x400 + 0x100 + 4 + 4 + 128 * 8)
DES_bs_all_possible_alignment_gaps:
.space 0x100
# 1039 "x86-sse.S"
.text

.align (1 << (5))
.globl DES_bs_init_asm
DES_bs_init_asm:
 pcmpeqd %xmm0,%xmm0
 movdqa %xmm0,DES_bs_all_tmp+(0)*16
 ret




.align (1 << (5))
.globl DES_bs_crypt
DES_bs_crypt:
 movl 4(%esp),%eax
 pxor %xmm0,%xmm0
 pushl %ebp
 pushl %esi
 movl $DES_bs_all_KS_v,%edx
 movdqa %xmm0,DES_bs_all_B+(0)*16; movdqa %xmm0,DES_bs_all_B+(0 + 1)*16; movdqa %xmm0,DES_bs_all_B+(0 + 2)*16; movdqa %xmm0,DES_bs_all_B+(0 + 3)*16; movdqa %xmm0,DES_bs_all_B+(0 + 4)*16; movdqa %xmm0,DES_bs_all_B+(0 + 5)*16; movdqa %xmm0,DES_bs_all_B+(0 + 6)*16; movdqa %xmm0,DES_bs_all_B+(0 + 7)*16; movdqa %xmm0,DES_bs_all_B+(8)*16; movdqa %xmm0,DES_bs_all_B+(8 + 1)*16; movdqa %xmm0,DES_bs_all_B+(8 + 2)*16; movdqa %xmm0,DES_bs_all_B+(8 + 3)*16; movdqa %xmm0,DES_bs_all_B+(8 + 4)*16; movdqa %xmm0,DES_bs_all_B+(8 + 5)*16; movdqa %xmm0,DES_bs_all_B+(8 + 6)*16; movdqa %xmm0,DES_bs_all_B+(8 + 7)*16; movdqa %xmm0,DES_bs_all_B+(16)*16; movdqa %xmm0,DES_bs_all_B+(16 + 1)*16; movdqa %xmm0,DES_bs_all_B+(16 + 2)*16; movdqa %xmm0,DES_bs_all_B+(16 + 3)*16; movdqa %xmm0,DES_bs_all_B+(16 + 4)*16; movdqa %xmm0,DES_bs_all_B+(16 + 5)*16; movdqa %xmm0,DES_bs_all_B+(16 + 6)*16; movdqa %xmm0,DES_bs_all_B+(16 + 7)*16; movdqa %xmm0,DES_bs_all_B+(24)*16; movdqa %xmm0,DES_bs_all_B+(24 + 1)*16; movdqa %xmm0,DES_bs_all_B+(24 + 2)*16; movdqa %xmm0,DES_bs_all_B+(24 + 3)*16; movdqa %xmm0,DES_bs_all_B+(24 + 4)*16; movdqa %xmm0,DES_bs_all_B+(24 + 5)*16; movdqa %xmm0,DES_bs_all_B+(24 + 6)*16; movdqa %xmm0,DES_bs_all_B+(24 + 7)*16; movdqa %xmm0,DES_bs_all_B+(32)*16; movdqa %xmm0,DES_bs_all_B+(32 + 1)*16; movdqa %xmm0,DES_bs_all_B+(32 + 2)*16; movdqa %xmm0,DES_bs_all_B+(32 + 3)*16; movdqa %xmm0,DES_bs_all_B+(32 + 4)*16; movdqa %xmm0,DES_bs_all_B+(32 + 5)*16; movdqa %xmm0,DES_bs_all_B+(32 + 6)*16; movdqa %xmm0,DES_bs_all_B+(32 + 7)*16; movdqa %xmm0,DES_bs_all_B+(40)*16; movdqa %xmm0,DES_bs_all_B+(40 + 1)*16; movdqa %xmm0,DES_bs_all_B+(40 + 2)*16; movdqa %xmm0,DES_bs_all_B+(40 + 3)*16; movdqa %xmm0,DES_bs_all_B+(40 + 4)*16; movdqa %xmm0,DES_bs_all_B+(40 + 5)*16; movdqa %xmm0,DES_bs_all_B+(40 + 6)*16; movdqa %xmm0,DES_bs_all_B+(40 + 7)*16; movdqa %xmm0,DES_bs_all_B+(48)*16; movdqa %xmm0,DES_bs_all_B+(48 + 1)*16; movdqa %xmm0,DES_bs_all_B+(48 + 2)*16; movdqa %xmm0,DES_bs_all_B+(48 + 3)*16; movdqa %xmm0,DES_bs_all_B+(48 + 4)*16; movdqa %xmm0,DES_bs_all_B+(48 + 5)*16; movdqa %xmm0,DES_bs_all_B+(48 + 6)*16; movdqa %xmm0,DES_bs_all_B+(48 + 7)*16; movdqa %xmm0,DES_bs_all_B+(56)*16; movdqa %xmm0,DES_bs_all_B+(56 + 1)*16; movdqa %xmm0,DES_bs_all_B+(56 + 2)*16; movdqa %xmm0,DES_bs_all_B+(56 + 3)*16; movdqa %xmm0,DES_bs_all_B+(56 + 4)*16; movdqa %xmm0,DES_bs_all_B+(56 + 5)*16; movdqa %xmm0,DES_bs_all_B+(56 + 6)*16; movdqa %xmm0,DES_bs_all_B+(56 + 7)*16
 movl $8,%ebp
DES_bs_crypt_start:
 movl DES_bs_all_E+(0)*4,%ecx; movdqa (0)*16(%edx),%xmm0; movl DES_bs_all_E+(0 + 1)*4,%esi; movdqa (0 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(0 + 2)*4,%ecx; movdqa (0 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(0 + 3)*4,%esi; movdqa (0 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(0 + 4)*4,%ecx; movdqa (0 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(0 + 5)*4,%esi; movdqa (0 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(40)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(40)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(62)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(48)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(62)*16; pxor DES_bs_all_B+(54)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(48)*16; movdqa %xmm2,DES_bs_all_B+(54)*16
 movl DES_bs_all_E+(6)*4,%ecx; movdqa (6)*16(%edx),%xmm0; movl DES_bs_all_E+(6 + 1)*4,%esi; movdqa (6 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(6 + 2)*4,%ecx; movdqa (6 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(6 + 3)*4,%esi; movdqa (6 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(6 + 4)*4,%ecx; movdqa (6 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(6 + 5)*4,%esi; movdqa (6 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(59)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(59)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(33)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(44)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(49)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(44)*16; movdqa %xmm7,DES_bs_all_B+(33)*16; movdqa %xmm2,DES_bs_all_B+(49)*16
 movl DES_bs_all_E+(12)*4,%ecx; movdqa (12)*16(%edx),%xmm0; movl DES_bs_all_E+(12 + 1)*4,%esi; movdqa (12 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(12 + 2)*4,%ecx; movdqa (12 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(12 + 3)*4,%esi; movdqa (12 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(12 + 4)*4,%ecx; movdqa (12 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(12 + 5)*4,%esi; movdqa (12 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(37)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(37)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(61)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(55)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(61)*16; pxor DES_bs_all_B+(47)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(55)*16; movdqa %xmm6,DES_bs_all_B+(47)*16
 movl DES_bs_all_E+(18)*4,%ecx; movdqa (18)*16(%edx),%xmm0; movl DES_bs_all_E+(18 + 1)*4,%esi; movdqa (18 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(18 + 2)*4,%ecx; movdqa (18 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(18 + 3)*4,%esi; movdqa (18 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(18 + 4)*4,%ecx; movdqa (18 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(18 + 5)*4,%esi; movdqa (18 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (%esi),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(57)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(41)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(51)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(57)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(51)*16; pxor DES_bs_all_B+(32)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(41)*16; movdqa %xmm5,DES_bs_all_B+(32)*16
 movl DES_bs_all_E+(24)*4,%ecx; movdqa (24)*16(%edx),%xmm0; movl DES_bs_all_E+(24 + 1)*4,%esi; movdqa (24 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(24 + 2)*4,%ecx; movdqa (24 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(24 + 3)*4,%esi; movdqa (24 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(24 + 4)*4,%ecx; movdqa (24 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(24 + 5)*4,%esi; movdqa (24 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(56)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(45)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(39)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(45)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(56)*16; pxor DES_bs_all_B+(34)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(34)*16; movdqa %xmm5,DES_bs_all_B+(39)*16
 movl DES_bs_all_E+(30)*4,%ecx; movdqa (30)*16(%edx),%xmm0; movl DES_bs_all_E+(30 + 1)*4,%esi; movdqa (30 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(30 + 2)*4,%ecx; movdqa (30 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(30 + 3)*4,%esi; movdqa (30 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(30 + 4)*4,%ecx; movdqa (30 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(30 + 5)*4,%esi; movdqa (30 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(60)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(60)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(50)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(35)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(50)*16; pxor DES_bs_all_B+(42)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(35)*16; movdqa %xmm2,DES_bs_all_B+(42)*16
 movl DES_bs_all_E+(36)*4,%ecx; movdqa (36)*16(%edx),%xmm0; movl DES_bs_all_E+(36 + 1)*4,%esi; movdqa (36 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(36 + 2)*4,%ecx; movdqa (36 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(36 + 3)*4,%esi; movdqa (36 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(36 + 4)*4,%ecx; movdqa (36 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(36 + 5)*4,%esi; movdqa (36 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (%esi),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(63)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(43)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(63)*16; pxor DES_bs_all_B+(53)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(43)*16; pxor DES_bs_all_B+(38)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(53)*16; movdqa %xmm0,DES_bs_all_B+(38)*16
 movl DES_bs_all_E+(42)*4,%ecx; movdqa (42)*16(%edx),%xmm0; movl DES_bs_all_E+(42 + 1)*4,%esi; movdqa (42 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(42 + 2)*4,%ecx; movdqa (42 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(42 + 3)*4,%esi; movdqa (42 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(42 + 4)*4,%ecx; movdqa (42 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(42 + 5)*4,%esi; movdqa (42 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(36)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(52)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(46)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(36)*16; pxor DES_bs_all_B+(58)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(52)*16; movdqa %xmm5,DES_bs_all_B+(46)*16; movdqa %xmm2,DES_bs_all_B+(58)*16
 cmpl $0x100,%ebp
 je DES_bs_crypt_next
DES_bs_crypt_swap:
 movl DES_bs_all_E+(48)*4,%ecx; movdqa (48)*16(%edx),%xmm0; movl DES_bs_all_E+(48 + 1)*4,%esi; movdqa (48 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(48 + 2)*4,%ecx; movdqa (48 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(48 + 3)*4,%esi; movdqa (48 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(48 + 4)*4,%ecx; movdqa (48 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(48 + 5)*4,%esi; movdqa (48 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(8)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(8)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(30)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(16)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(30)*16; pxor DES_bs_all_B+(22)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(16)*16; movdqa %xmm2,DES_bs_all_B+(22)*16
 movl DES_bs_all_E+(54)*4,%ecx; movdqa (54)*16(%edx),%xmm0; movl DES_bs_all_E+(54 + 1)*4,%esi; movdqa (54 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(54 + 2)*4,%ecx; movdqa (54 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(54 + 3)*4,%esi; movdqa (54 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(54 + 4)*4,%ecx; movdqa (54 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(54 + 5)*4,%esi; movdqa (54 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(27)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(27)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(1)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(12)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(17)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(12)*16; movdqa %xmm7,DES_bs_all_B+(1)*16; movdqa %xmm2,DES_bs_all_B+(17)*16
 movl DES_bs_all_E+(60)*4,%ecx; movdqa (60)*16(%edx),%xmm0; movl DES_bs_all_E+(60 + 1)*4,%esi; movdqa (60 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(60 + 2)*4,%ecx; movdqa (60 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(60 + 3)*4,%esi; movdqa (60 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(60 + 4)*4,%ecx; movdqa (60 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(60 + 5)*4,%esi; movdqa (60 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(5)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(5)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(29)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(23)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(29)*16; pxor DES_bs_all_B+(15)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(23)*16; movdqa %xmm6,DES_bs_all_B+(15)*16
 movl DES_bs_all_E+(66)*4,%ecx; movdqa (66)*16(%edx),%xmm0; movl DES_bs_all_E+(66 + 1)*4,%esi; movdqa (66 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(66 + 2)*4,%ecx; movdqa (66 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(66 + 3)*4,%esi; movdqa (66 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(66 + 4)*4,%ecx; movdqa (66 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(66 + 5)*4,%esi; movdqa (66 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (%esi),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(25)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(9)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(19)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(25)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(19)*16; pxor DES_bs_all_B+(0)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(9)*16; movdqa %xmm5,DES_bs_all_B+(0)*16
 movl DES_bs_all_E+(72)*4,%ecx; movdqa (72)*16(%edx),%xmm0; movl DES_bs_all_E+(72 + 1)*4,%esi; movdqa (72 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(72 + 2)*4,%ecx; movdqa (72 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(72 + 3)*4,%esi; movdqa (72 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(72 + 4)*4,%ecx; movdqa (72 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(72 + 5)*4,%esi; movdqa (72 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(24)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(13)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(7)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(13)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(24)*16; pxor DES_bs_all_B+(2)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(2)*16; movdqa %xmm5,DES_bs_all_B+(7)*16
 movl DES_bs_all_E+(78)*4,%ecx; movdqa (78)*16(%edx),%xmm0; movl DES_bs_all_E+(78 + 1)*4,%esi; movdqa (78 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(78 + 2)*4,%ecx; movdqa (78 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(78 + 3)*4,%esi; movdqa (78 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(78 + 4)*4,%ecx; movdqa (78 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(78 + 5)*4,%esi; movdqa (78 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(28)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(28)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(18)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(3)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(18)*16; pxor DES_bs_all_B+(10)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(3)*16; movdqa %xmm2,DES_bs_all_B+(10)*16
 movl DES_bs_all_E+(84)*4,%ecx; movdqa (84)*16(%edx),%xmm0; movl DES_bs_all_E+(84 + 1)*4,%esi; movdqa (84 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(84 + 2)*4,%ecx; movdqa (84 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(84 + 3)*4,%esi; movdqa (84 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(84 + 4)*4,%ecx; movdqa (84 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(84 + 5)*4,%esi; movdqa (84 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (%esi),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(31)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(11)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(31)*16; pxor DES_bs_all_B+(21)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(11)*16; pxor DES_bs_all_B+(6)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(21)*16; movdqa %xmm0,DES_bs_all_B+(6)*16
 movl DES_bs_all_E+(90)*4,%ecx; movdqa (90)*16(%edx),%xmm0; movl DES_bs_all_E+(90 + 1)*4,%esi; movdqa (90 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(90 + 2)*4,%ecx; movdqa (90 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(90 + 3)*4,%esi; movdqa (90 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(90 + 4)*4,%ecx; movdqa (90 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(90 + 5)*4,%esi; movdqa (90 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 addl $96*16,%edx
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(4)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(20)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(14)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(4)*16; pxor DES_bs_all_B+(26)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(20)*16; movdqa %xmm5,DES_bs_all_B+(14)*16; movdqa %xmm2,DES_bs_all_B+(26)*16
 decl %ebp
 jnz DES_bs_crypt_start
 subl $0x300*16+48*16,%edx
 movl $0x108,%ebp
 decl %eax
 jnz DES_bs_crypt_swap
 popl %esi
 popl %ebp
 ret
DES_bs_crypt_next:
 subl $0x300*16-48*16,%edx
 movl $8,%ebp
 decl %eax
 jnz DES_bs_crypt_start
 popl %esi
 popl %ebp
 ret

.align (1 << (5))
.globl DES_bs_crypt_25
DES_bs_crypt_25:
 pxor %xmm0,%xmm0
 pushl %ebp
 pushl %esi
 movl $DES_bs_all_KS_v,%edx
 movdqa %xmm0,DES_bs_all_B+(0)*16; movdqa %xmm0,DES_bs_all_B+(0 + 1)*16; movdqa %xmm0,DES_bs_all_B+(0 + 2)*16; movdqa %xmm0,DES_bs_all_B+(0 + 3)*16; movdqa %xmm0,DES_bs_all_B+(0 + 4)*16; movdqa %xmm0,DES_bs_all_B+(0 + 5)*16; movdqa %xmm0,DES_bs_all_B+(0 + 6)*16; movdqa %xmm0,DES_bs_all_B+(0 + 7)*16; movdqa %xmm0,DES_bs_all_B+(8)*16; movdqa %xmm0,DES_bs_all_B+(8 + 1)*16; movdqa %xmm0,DES_bs_all_B+(8 + 2)*16; movdqa %xmm0,DES_bs_all_B+(8 + 3)*16; movdqa %xmm0,DES_bs_all_B+(8 + 4)*16; movdqa %xmm0,DES_bs_all_B+(8 + 5)*16; movdqa %xmm0,DES_bs_all_B+(8 + 6)*16; movdqa %xmm0,DES_bs_all_B+(8 + 7)*16; movdqa %xmm0,DES_bs_all_B+(16)*16; movdqa %xmm0,DES_bs_all_B+(16 + 1)*16; movdqa %xmm0,DES_bs_all_B+(16 + 2)*16; movdqa %xmm0,DES_bs_all_B+(16 + 3)*16; movdqa %xmm0,DES_bs_all_B+(16 + 4)*16; movdqa %xmm0,DES_bs_all_B+(16 + 5)*16; movdqa %xmm0,DES_bs_all_B+(16 + 6)*16; movdqa %xmm0,DES_bs_all_B+(16 + 7)*16; movdqa %xmm0,DES_bs_all_B+(24)*16; movdqa %xmm0,DES_bs_all_B+(24 + 1)*16; movdqa %xmm0,DES_bs_all_B+(24 + 2)*16; movdqa %xmm0,DES_bs_all_B+(24 + 3)*16; movdqa %xmm0,DES_bs_all_B+(24 + 4)*16; movdqa %xmm0,DES_bs_all_B+(24 + 5)*16; movdqa %xmm0,DES_bs_all_B+(24 + 6)*16; movdqa %xmm0,DES_bs_all_B+(24 + 7)*16; movdqa %xmm0,DES_bs_all_B+(32)*16; movdqa %xmm0,DES_bs_all_B+(32 + 1)*16; movdqa %xmm0,DES_bs_all_B+(32 + 2)*16; movdqa %xmm0,DES_bs_all_B+(32 + 3)*16; movdqa %xmm0,DES_bs_all_B+(32 + 4)*16; movdqa %xmm0,DES_bs_all_B+(32 + 5)*16; movdqa %xmm0,DES_bs_all_B+(32 + 6)*16; movdqa %xmm0,DES_bs_all_B+(32 + 7)*16; movdqa %xmm0,DES_bs_all_B+(40)*16; movdqa %xmm0,DES_bs_all_B+(40 + 1)*16; movdqa %xmm0,DES_bs_all_B+(40 + 2)*16; movdqa %xmm0,DES_bs_all_B+(40 + 3)*16; movdqa %xmm0,DES_bs_all_B+(40 + 4)*16; movdqa %xmm0,DES_bs_all_B+(40 + 5)*16; movdqa %xmm0,DES_bs_all_B+(40 + 6)*16; movdqa %xmm0,DES_bs_all_B+(40 + 7)*16; movdqa %xmm0,DES_bs_all_B+(48)*16; movdqa %xmm0,DES_bs_all_B+(48 + 1)*16; movdqa %xmm0,DES_bs_all_B+(48 + 2)*16; movdqa %xmm0,DES_bs_all_B+(48 + 3)*16; movdqa %xmm0,DES_bs_all_B+(48 + 4)*16; movdqa %xmm0,DES_bs_all_B+(48 + 5)*16; movdqa %xmm0,DES_bs_all_B+(48 + 6)*16; movdqa %xmm0,DES_bs_all_B+(48 + 7)*16; movdqa %xmm0,DES_bs_all_B+(56)*16; movdqa %xmm0,DES_bs_all_B+(56 + 1)*16; movdqa %xmm0,DES_bs_all_B+(56 + 2)*16; movdqa %xmm0,DES_bs_all_B+(56 + 3)*16; movdqa %xmm0,DES_bs_all_B+(56 + 4)*16; movdqa %xmm0,DES_bs_all_B+(56 + 5)*16; movdqa %xmm0,DES_bs_all_B+(56 + 6)*16; movdqa %xmm0,DES_bs_all_B+(56 + 7)*16
 movl $8,%ebp
 movl $25,%eax
DES_bs_crypt_25_start:
 movl DES_bs_all_E+(0)*4,%ecx; movdqa (0)*16(%edx),%xmm0; movl DES_bs_all_E+(0 + 1)*4,%esi; movdqa (0 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(0 + 2)*4,%ecx; movdqa (0 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(0 + 3)*4,%esi; movdqa (0 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(0 + 4)*4,%ecx; movdqa (0 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(0 + 5)*4,%esi; movdqa (0 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(40)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(40)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(62)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(48)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(62)*16; pxor DES_bs_all_B+(54)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(48)*16; movdqa %xmm2,DES_bs_all_B+(54)*16
 movl DES_bs_all_E+(6)*4,%ecx; movdqa (6)*16(%edx),%xmm0; movl DES_bs_all_E+(6 + 1)*4,%esi; movdqa (6 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(6 + 2)*4,%ecx; movdqa (6 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(6 + 3)*4,%esi; movdqa (6 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(6 + 4)*4,%ecx; movdqa (6 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(6 + 5)*4,%esi; movdqa (6 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(59)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(59)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(33)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(44)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(49)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(44)*16; movdqa %xmm7,DES_bs_all_B+(33)*16; movdqa %xmm2,DES_bs_all_B+(49)*16
 movdqa DES_bs_all_B+(7)*16,%xmm0; movdqa DES_bs_all_B+(8)*16,%xmm1; pxor (12)*16(%edx),%xmm0; movdqa DES_bs_all_B+(9)*16,%xmm2; pxor (13)*16(%edx),%xmm1; movdqa DES_bs_all_B+(10)*16,%xmm3; pxor (14)*16(%edx),%xmm2; movdqa DES_bs_all_B+(11)*16,%xmm4; pxor (15)*16(%edx),%xmm3; movdqa DES_bs_all_B+(12)*16,%xmm5; pxor (16)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (17)*16(%edx),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(37)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(37)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(61)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(55)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(61)*16; pxor DES_bs_all_B+(47)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(55)*16; movdqa %xmm6,DES_bs_all_B+(47)*16
 movdqa DES_bs_all_B+(11)*16,%xmm0; movdqa DES_bs_all_B+(12)*16,%xmm1; pxor (18)*16(%edx),%xmm0; movdqa DES_bs_all_B+(13)*16,%xmm2; pxor (19)*16(%edx),%xmm1; movdqa DES_bs_all_B+(14)*16,%xmm3; pxor (20)*16(%edx),%xmm2; movdqa DES_bs_all_B+(15)*16,%xmm4; pxor (21)*16(%edx),%xmm3; movdqa DES_bs_all_B+(16)*16,%xmm5; pxor (22)*16(%edx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (23)*16(%edx),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(57)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(41)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(51)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(57)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(51)*16; pxor DES_bs_all_B+(32)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(41)*16; movdqa %xmm5,DES_bs_all_B+(32)*16
 movl DES_bs_all_E+(24)*4,%ecx; movdqa (24)*16(%edx),%xmm0; movl DES_bs_all_E+(24 + 1)*4,%esi; movdqa (24 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(24 + 2)*4,%ecx; movdqa (24 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(24 + 3)*4,%esi; movdqa (24 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(24 + 4)*4,%ecx; movdqa (24 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(24 + 5)*4,%esi; movdqa (24 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(56)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(45)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(39)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(45)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(56)*16; pxor DES_bs_all_B+(34)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(34)*16; movdqa %xmm5,DES_bs_all_B+(39)*16
 movl DES_bs_all_E+(30)*4,%ecx; movdqa (30)*16(%edx),%xmm0; movl DES_bs_all_E+(30 + 1)*4,%esi; movdqa (30 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(30 + 2)*4,%ecx; movdqa (30 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(30 + 3)*4,%esi; movdqa (30 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(30 + 4)*4,%ecx; movdqa (30 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(30 + 5)*4,%esi; movdqa (30 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(60)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(60)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(50)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(35)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(50)*16; pxor DES_bs_all_B+(42)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(35)*16; movdqa %xmm2,DES_bs_all_B+(42)*16
 movdqa DES_bs_all_B+(23)*16,%xmm0; movdqa DES_bs_all_B+(24)*16,%xmm1; pxor (36)*16(%edx),%xmm0; movdqa DES_bs_all_B+(25)*16,%xmm2; pxor (37)*16(%edx),%xmm1; movdqa DES_bs_all_B+(26)*16,%xmm3; pxor (38)*16(%edx),%xmm2; movdqa DES_bs_all_B+(27)*16,%xmm4; pxor (39)*16(%edx),%xmm3; movdqa DES_bs_all_B+(28)*16,%xmm5; pxor (40)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (41)*16(%edx),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(63)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(43)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(63)*16; pxor DES_bs_all_B+(53)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(43)*16; pxor DES_bs_all_B+(38)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(53)*16; movdqa %xmm0,DES_bs_all_B+(38)*16
 movdqa DES_bs_all_B+(27)*16,%xmm0; movdqa DES_bs_all_B+(28)*16,%xmm1; pxor (42)*16(%edx),%xmm0; movdqa DES_bs_all_B+(29)*16,%xmm2; pxor (43)*16(%edx),%xmm1; movdqa DES_bs_all_B+(30)*16,%xmm3; pxor (44)*16(%edx),%xmm2; movdqa DES_bs_all_B+(31)*16,%xmm4; pxor (45)*16(%edx),%xmm3; movdqa DES_bs_all_B+(0)*16,%xmm5; pxor (46)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (47)*16(%edx),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(36)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(52)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(46)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(36)*16; pxor DES_bs_all_B+(58)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(52)*16; movdqa %xmm5,DES_bs_all_B+(46)*16; movdqa %xmm2,DES_bs_all_B+(58)*16
 cmpl $0x100,%ebp
 je DES_bs_crypt_25_next
DES_bs_crypt_25_swap:
 movl DES_bs_all_E+(48)*4,%ecx; movdqa (48)*16(%edx),%xmm0; movl DES_bs_all_E+(48 + 1)*4,%esi; movdqa (48 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(48 + 2)*4,%ecx; movdqa (48 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(48 + 3)*4,%esi; movdqa (48 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(48 + 4)*4,%ecx; movdqa (48 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(48 + 5)*4,%esi; movdqa (48 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(8)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(8)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(30)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(16)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(30)*16; pxor DES_bs_all_B+(22)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(16)*16; movdqa %xmm2,DES_bs_all_B+(22)*16
 movl DES_bs_all_E+(54)*4,%ecx; movdqa (54)*16(%edx),%xmm0; movl DES_bs_all_E+(54 + 1)*4,%esi; movdqa (54 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(54 + 2)*4,%ecx; movdqa (54 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(54 + 3)*4,%esi; movdqa (54 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(54 + 4)*4,%ecx; movdqa (54 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(54 + 5)*4,%esi; movdqa (54 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(27)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(27)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(1)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(12)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(17)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(12)*16; movdqa %xmm7,DES_bs_all_B+(1)*16; movdqa %xmm2,DES_bs_all_B+(17)*16
 movdqa DES_bs_all_B+(39)*16,%xmm0; movdqa DES_bs_all_B+(40)*16,%xmm1; pxor (60)*16(%edx),%xmm0; movdqa DES_bs_all_B+(41)*16,%xmm2; pxor (61)*16(%edx),%xmm1; movdqa DES_bs_all_B+(42)*16,%xmm3; pxor (62)*16(%edx),%xmm2; movdqa DES_bs_all_B+(43)*16,%xmm4; pxor (63)*16(%edx),%xmm3; movdqa DES_bs_all_B+(44)*16,%xmm5; pxor (64)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (65)*16(%edx),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(5)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(5)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(29)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(23)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(29)*16; pxor DES_bs_all_B+(15)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(23)*16; movdqa %xmm6,DES_bs_all_B+(15)*16
 movdqa DES_bs_all_B+(43)*16,%xmm0; movdqa DES_bs_all_B+(44)*16,%xmm1; pxor (66)*16(%edx),%xmm0; movdqa DES_bs_all_B+(45)*16,%xmm2; pxor (67)*16(%edx),%xmm1; movdqa DES_bs_all_B+(46)*16,%xmm3; pxor (68)*16(%edx),%xmm2; movdqa DES_bs_all_B+(47)*16,%xmm4; pxor (69)*16(%edx),%xmm3; movdqa DES_bs_all_B+(48)*16,%xmm5; pxor (70)*16(%edx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (71)*16(%edx),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(25)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(9)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(19)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(25)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(19)*16; pxor DES_bs_all_B+(0)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(9)*16; movdqa %xmm5,DES_bs_all_B+(0)*16
 movl DES_bs_all_E+(72)*4,%ecx; movdqa (72)*16(%edx),%xmm0; movl DES_bs_all_E+(72 + 1)*4,%esi; movdqa (72 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(72 + 2)*4,%ecx; movdqa (72 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(72 + 3)*4,%esi; movdqa (72 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(72 + 4)*4,%ecx; movdqa (72 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(72 + 5)*4,%esi; movdqa (72 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(24)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(13)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(7)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(13)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(24)*16; pxor DES_bs_all_B+(2)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(2)*16; movdqa %xmm5,DES_bs_all_B+(7)*16
 movl DES_bs_all_E+(78)*4,%ecx; movdqa (78)*16(%edx),%xmm0; movl DES_bs_all_E+(78 + 1)*4,%esi; movdqa (78 + 1)*16(%edx),%xmm1; pxor (%ecx),%xmm0; pxor (%esi),%xmm1; movl DES_bs_all_E+(78 + 2)*4,%ecx; movdqa (78 + 2)*16(%edx),%xmm2; movl DES_bs_all_E+(78 + 3)*4,%esi; movdqa (78 + 3)*16(%edx),%xmm3; pxor (%ecx),%xmm2; pxor (%esi),%xmm3; movl DES_bs_all_E+(78 + 4)*4,%ecx; movdqa (78 + 4)*16(%edx),%xmm4; movl DES_bs_all_E+(78 + 5)*4,%esi; movdqa (78 + 5)*16(%edx),%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(28)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(28)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(18)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(3)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(18)*16; pxor DES_bs_all_B+(10)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(3)*16; movdqa %xmm2,DES_bs_all_B+(10)*16
 movdqa DES_bs_all_B+(55)*16,%xmm0; movdqa DES_bs_all_B+(56)*16,%xmm1; pxor (84)*16(%edx),%xmm0; movdqa DES_bs_all_B+(57)*16,%xmm2; pxor (85)*16(%edx),%xmm1; movdqa DES_bs_all_B+(58)*16,%xmm3; pxor (86)*16(%edx),%xmm2; movdqa DES_bs_all_B+(59)*16,%xmm4; pxor (87)*16(%edx),%xmm3; movdqa DES_bs_all_B+(60)*16,%xmm5; pxor (88)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (89)*16(%edx),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(31)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(11)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(31)*16; pxor DES_bs_all_B+(21)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(11)*16; pxor DES_bs_all_B+(6)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(21)*16; movdqa %xmm0,DES_bs_all_B+(6)*16
 movdqa DES_bs_all_B+(59)*16,%xmm0; movdqa DES_bs_all_B+(60)*16,%xmm1; pxor (90)*16(%edx),%xmm0; movdqa DES_bs_all_B+(61)*16,%xmm2; pxor (91)*16(%edx),%xmm1; movdqa DES_bs_all_B+(62)*16,%xmm3; pxor (92)*16(%edx),%xmm2; movdqa DES_bs_all_B+(63)*16,%xmm4; pxor (93)*16(%edx),%xmm3; movdqa DES_bs_all_B+(32)*16,%xmm5; pxor (94)*16(%edx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (95)*16(%edx),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(4)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(20)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(14)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(4)*16; pxor DES_bs_all_B+(26)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(20)*16; movdqa %xmm5,DES_bs_all_B+(14)*16; movdqa %xmm2,DES_bs_all_B+(26)*16
 addl $96*16,%edx
 decl %ebp
 jnz DES_bs_crypt_25_start
 subl $0x300*16+48*16,%edx
 movl $0x108,%ebp
 decl %eax
 jnz DES_bs_crypt_25_swap
 popl %esi
 popl %ebp
 ret
DES_bs_crypt_25_next:
 subl $0x300*16-48*16,%edx
 movl $8,%ebp
 decl %eax
 jmp DES_bs_crypt_25_start





.align (1 << (5))
.globl DES_bs_crypt_LM
DES_bs_crypt_LM:
 pxor %xmm0,%xmm0
 pushl %esi
 pcmpeqd %xmm1,%xmm1
 movl $DES_bs_all_KS_p,%edx
 movdqa %xmm0,DES_bs_all_B+(0)*16
 movdqa %xmm0,DES_bs_all_B+(1)*16
 movdqa %xmm0,DES_bs_all_B+(2)*16
 movdqa %xmm0,DES_bs_all_B+(3)*16
 movdqa %xmm0,DES_bs_all_B+(4)*16
 movdqa %xmm0,DES_bs_all_B+(5)*16
 movdqa %xmm0,DES_bs_all_B+(6)*16
 movdqa %xmm0,DES_bs_all_B+(7)*16
 movdqa %xmm1,DES_bs_all_B+(8)*16
 movdqa %xmm1,DES_bs_all_B+(9)*16
 movdqa %xmm1,DES_bs_all_B+(10)*16
 movdqa %xmm0,DES_bs_all_B+(11)*16
 movdqa %xmm1,DES_bs_all_B+(12)*16
 movdqa %xmm0,DES_bs_all_B+(13)*16
 movdqa %xmm0,DES_bs_all_B+(14)*16
 movdqa %xmm0,DES_bs_all_B+(15)*16
 movdqa %xmm0,DES_bs_all_B+(16)*16
 movdqa %xmm0,DES_bs_all_B+(17)*16
 movdqa %xmm0,DES_bs_all_B+(18)*16
 movdqa %xmm0,DES_bs_all_B+(19)*16
 movdqa %xmm0,DES_bs_all_B+(20)*16
 movdqa %xmm0,DES_bs_all_B+(21)*16
 movdqa %xmm0,DES_bs_all_B+(22)*16
 movdqa %xmm1,DES_bs_all_B+(23)*16
 movdqa %xmm0,DES_bs_all_B+(24)*16
 movdqa %xmm0,DES_bs_all_B+(25)*16
 movdqa %xmm1,DES_bs_all_B+(26)*16
 movdqa %xmm0,DES_bs_all_B+(27)*16
 movdqa %xmm0,DES_bs_all_B+(28)*16
 movdqa %xmm1,DES_bs_all_B+(29)*16
 movdqa %xmm1,DES_bs_all_B+(30)*16
 movdqa %xmm1,DES_bs_all_B+(31)*16
 movdqa %xmm0,DES_bs_all_B+(32)*16
 movdqa %xmm0,DES_bs_all_B+(33)*16
 movdqa %xmm0,DES_bs_all_B+(34)*16
 movdqa %xmm1,DES_bs_all_B+(35)*16
 movdqa %xmm0,DES_bs_all_B+(36)*16
 movdqa %xmm1,DES_bs_all_B+(37)*16
 movdqa %xmm1,DES_bs_all_B+(38)*16
 movdqa %xmm1,DES_bs_all_B+(39)*16
 movdqa %xmm0,DES_bs_all_B+(40)*16
 movdqa %xmm0,DES_bs_all_B+(41)*16
 movdqa %xmm0,DES_bs_all_B+(42)*16
 movdqa %xmm0,DES_bs_all_B+(43)*16
 movdqa %xmm0,DES_bs_all_B+(44)*16
 movdqa %xmm1,DES_bs_all_B+(45)*16
 movdqa %xmm0,DES_bs_all_B+(46)*16
 movdqa %xmm0,DES_bs_all_B+(47)*16
 movdqa %xmm1,DES_bs_all_B+(48)*16
 movdqa %xmm1,DES_bs_all_B+(49)*16
 movdqa %xmm0,DES_bs_all_B+(50)*16
 movdqa %xmm0,DES_bs_all_B+(51)*16
 movdqa %xmm0,DES_bs_all_B+(52)*16
 movdqa %xmm0,DES_bs_all_B+(53)*16
 movdqa %xmm1,DES_bs_all_B+(54)*16
 movdqa %xmm0,DES_bs_all_B+(55)*16
 movdqa %xmm1,DES_bs_all_B+(56)*16
 movdqa %xmm0,DES_bs_all_B+(57)*16
 movdqa %xmm1,DES_bs_all_B+(58)*16
 movdqa %xmm0,DES_bs_all_B+(59)*16
 movdqa %xmm1,DES_bs_all_B+(60)*16
 movdqa %xmm1,DES_bs_all_B+(61)*16
 movdqa %xmm1,DES_bs_all_B+(62)*16
 movdqa %xmm1,DES_bs_all_B+(63)*16
 movl $8,%eax
DES_bs_crypt_LM_loop:
 movl (0)*4(%edx),%ecx; movl (1)*4(%edx),%esi; movdqa DES_bs_all_B+(31)*16,%xmm0; movdqa DES_bs_all_B+(0)*16,%xmm1; pxor (%ecx),%xmm0; movl (2)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (3)*4(%edx),%esi; movdqa DES_bs_all_B+(1)*16,%xmm2; movdqa DES_bs_all_B+(2)*16,%xmm3; pxor (%ecx),%xmm2; movl (4)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(3)*16,%xmm4; movl (5)*4(%edx),%esi; movdqa DES_bs_all_B+(4)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(40)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(40)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(62)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(48)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(62)*16; pxor DES_bs_all_B+(54)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(48)*16; movdqa %xmm2,DES_bs_all_B+(54)*16
 movl (6)*4(%edx),%ecx; movl (7)*4(%edx),%esi; movdqa DES_bs_all_B+(3)*16,%xmm0; movdqa DES_bs_all_B+(4)*16,%xmm1; pxor (%ecx),%xmm0; movl (8)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (9)*4(%edx),%esi; movdqa DES_bs_all_B+(5)*16,%xmm2; movdqa DES_bs_all_B+(6)*16,%xmm3; pxor (%ecx),%xmm2; movl (10)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(7)*16,%xmm4; movl (11)*4(%edx),%esi; movdqa DES_bs_all_B+(8)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(59)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(59)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(33)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(44)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(49)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(44)*16; movdqa %xmm7,DES_bs_all_B+(33)*16; movdqa %xmm2,DES_bs_all_B+(49)*16
 movl (12)*4(%edx),%ecx; movl (13)*4(%edx),%esi; movdqa DES_bs_all_B+(7)*16,%xmm0; movdqa DES_bs_all_B+(8)*16,%xmm1; pxor (%ecx),%xmm0; movl (14)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (15)*4(%edx),%esi; movdqa DES_bs_all_B+(9)*16,%xmm2; movdqa DES_bs_all_B+(10)*16,%xmm3; pxor (%ecx),%xmm2; movl (16)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(11)*16,%xmm4; movl (17)*4(%edx),%esi; movdqa DES_bs_all_B+(12)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(37)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(37)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(61)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(55)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(61)*16; pxor DES_bs_all_B+(47)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(55)*16; movdqa %xmm6,DES_bs_all_B+(47)*16
 movl (18)*4(%edx),%ecx; movl (19)*4(%edx),%esi; movdqa DES_bs_all_B+(11)*16,%xmm0; movdqa DES_bs_all_B+(12)*16,%xmm1; pxor (%ecx),%xmm0; movl (20)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (21)*4(%edx),%esi; movdqa DES_bs_all_B+(13)*16,%xmm2; movdqa DES_bs_all_B+(14)*16,%xmm3; pxor (%ecx),%xmm2; movl (22)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(15)*16,%xmm4; movl (23)*4(%edx),%esi; movdqa DES_bs_all_B+(16)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (%esi),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(57)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(41)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(51)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(57)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(51)*16; pxor DES_bs_all_B+(32)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(41)*16; movdqa %xmm5,DES_bs_all_B+(32)*16
 movl (24)*4(%edx),%ecx; movl (25)*4(%edx),%esi; movdqa DES_bs_all_B+(15)*16,%xmm0; movdqa DES_bs_all_B+(16)*16,%xmm1; pxor (%ecx),%xmm0; movl (26)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (27)*4(%edx),%esi; movdqa DES_bs_all_B+(17)*16,%xmm2; movdqa DES_bs_all_B+(18)*16,%xmm3; pxor (%ecx),%xmm2; movl (28)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(19)*16,%xmm4; movl (29)*4(%edx),%esi; movdqa DES_bs_all_B+(20)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(56)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(45)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(39)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(45)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(56)*16; pxor DES_bs_all_B+(34)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(34)*16; movdqa %xmm5,DES_bs_all_B+(39)*16
 movl (30)*4(%edx),%ecx; movl (31)*4(%edx),%esi; movdqa DES_bs_all_B+(19)*16,%xmm0; movdqa DES_bs_all_B+(20)*16,%xmm1; pxor (%ecx),%xmm0; movl (32)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (33)*4(%edx),%esi; movdqa DES_bs_all_B+(21)*16,%xmm2; movdqa DES_bs_all_B+(22)*16,%xmm3; pxor (%ecx),%xmm2; movl (34)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(23)*16,%xmm4; movl (35)*4(%edx),%esi; movdqa DES_bs_all_B+(24)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(60)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(60)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(50)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(35)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(50)*16; pxor DES_bs_all_B+(42)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(35)*16; movdqa %xmm2,DES_bs_all_B+(42)*16
 movl (36)*4(%edx),%ecx; movl (37)*4(%edx),%esi; movdqa DES_bs_all_B+(23)*16,%xmm0; movdqa DES_bs_all_B+(24)*16,%xmm1; pxor (%ecx),%xmm0; movl (38)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (39)*4(%edx),%esi; movdqa DES_bs_all_B+(25)*16,%xmm2; movdqa DES_bs_all_B+(26)*16,%xmm3; pxor (%ecx),%xmm2; movl (40)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(27)*16,%xmm4; movl (41)*4(%edx),%esi; movdqa DES_bs_all_B+(28)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (%esi),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(63)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(43)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(63)*16; pxor DES_bs_all_B+(53)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(43)*16; pxor DES_bs_all_B+(38)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(53)*16; movdqa %xmm0,DES_bs_all_B+(38)*16
 movl (42)*4(%edx),%ecx; movl (43)*4(%edx),%esi; movdqa DES_bs_all_B+(27)*16,%xmm0; movdqa DES_bs_all_B+(28)*16,%xmm1; pxor (%ecx),%xmm0; movl (44)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (45)*4(%edx),%esi; movdqa DES_bs_all_B+(29)*16,%xmm2; movdqa DES_bs_all_B+(30)*16,%xmm3; pxor (%ecx),%xmm2; movl (46)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(31)*16,%xmm4; movl (47)*4(%edx),%esi; movdqa DES_bs_all_B+(0)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(36)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(52)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(46)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(36)*16; pxor DES_bs_all_B+(58)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(52)*16; movdqa %xmm5,DES_bs_all_B+(46)*16; movdqa %xmm2,DES_bs_all_B+(58)*16
 movl (48)*4(%edx),%ecx; movl (49)*4(%edx),%esi; movdqa DES_bs_all_B+(63)*16,%xmm0; movdqa DES_bs_all_B+(32)*16,%xmm1; pxor (%ecx),%xmm0; movl (50)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (51)*4(%edx),%esi; movdqa DES_bs_all_B+(33)*16,%xmm2; movdqa DES_bs_all_B+(34)*16,%xmm3; pxor (%ecx),%xmm2; movl (52)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(35)*16,%xmm4; movl (53)*4(%edx),%esi; movdqa DES_bs_all_B+(36)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm3,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm0,%xmm7; pxor (%esi),%xmm5; movdqa %xmm4,DES_bs_all_tmp+(3)*16; por %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm4; movdqa %xmm6,DES_bs_all_tmp+(4)*16; pxor %xmm0,%xmm3; movdqa %xmm7,DES_bs_all_tmp+(7)*16; por %xmm6,%xmm0; movdqa %xmm2,DES_bs_all_tmp+(2)*16; pand %xmm6,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(6)*16; por %xmm3,%xmm2; pxor DES_bs_all_tmp+(0)*16,%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,DES_bs_all_tmp+(8)*16; por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pxor %xmm4,%xmm6; pand DES_bs_all_tmp+(2)*16,%xmm4; movdqa %xmm6,%xmm2; pxor DES_bs_all_tmp+(2)*16,%xmm6; por %xmm1,%xmm2; pand DES_bs_all_tmp+(7)*16,%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(13)*16; pxor %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(12)*16; movdqa %xmm5,%xmm4; movdqa %xmm2,DES_bs_all_tmp+(9)*16; por %xmm0,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(10)*16; movdqa %xmm3,%xmm2; pandn DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm7,%xmm4; por DES_bs_all_tmp+(6)*16,%xmm5; por %xmm1,%xmm0; pxor DES_bs_all_tmp+(13)*16,%xmm5; pxor %xmm0,%xmm4; movdqa DES_bs_all_tmp+(3)*16,%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(14)*16; por %xmm1,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm6; por %xmm4,%xmm0; pand DES_bs_all_tmp+(7)*16,%xmm6; por %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(15)*16; pxor %xmm3,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm7; movdqa %xmm1,%xmm5; pxor DES_bs_all_tmp+(12)*16,%xmm2; pand %xmm6,%xmm5; pand DES_bs_all_tmp+(2)*16,%xmm6; pxor %xmm7,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(6)*16,%xmm2; por %xmm3,%xmm7; por DES_bs_all_tmp+(13)*16,%xmm2; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm4; por %xmm1,%xmm7; por DES_bs_all_tmp+(12)*16,%xmm4; por %xmm1,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm4; pxor %xmm2,%xmm6; movdqa DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm4,%xmm7; pxor DES_bs_all_tmp+(14)*16,%xmm3; pxor %xmm2,%xmm0; pxor DES_bs_all_B+(8)*16,%xmm5; pand %xmm3,%xmm2; movdqa DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_B+(8)*16; pxor DES_bs_all_tmp+(15)*16,%xmm2; pand %xmm4,%xmm7; pxor DES_bs_all_B+(30)*16,%xmm0; pand %xmm4,%xmm2; pxor DES_bs_all_B+(16)*16,%xmm7; movdqa %xmm0,DES_bs_all_B+(30)*16; pxor DES_bs_all_B+(22)*16,%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,DES_bs_all_B+(16)*16; movdqa %xmm2,DES_bs_all_B+(22)*16
 movl (54)*4(%edx),%ecx; movl (55)*4(%edx),%esi; movdqa DES_bs_all_B+(35)*16,%xmm0; movdqa DES_bs_all_B+(36)*16,%xmm1; pxor (%ecx),%xmm0; movl (56)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (57)*4(%edx),%esi; movdqa DES_bs_all_B+(37)*16,%xmm2; movdqa DES_bs_all_B+(38)*16,%xmm3; pxor (%ecx),%xmm2; movl (58)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(39)*16,%xmm4; movl (59)*4(%edx),%esi; movdqa DES_bs_all_B+(40)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm3,DES_bs_all_tmp+(4)*16; movdqa %xmm4,%xmm6; pxor (%esi),%xmm5; movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm4,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(3)*16; por %xmm5,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(5)*16; por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,DES_bs_all_tmp+(2)*16; pxor %xmm5,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm1,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(8)*16; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm2,%xmm1; movdqa DES_bs_all_tmp+(6)*16,%xmm7; movdqa %xmm1,%xmm2; pand DES_bs_all_tmp+(4)*16,%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,DES_bs_all_tmp+(7)*16; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor DES_bs_all_B+(27)*16,%xmm3; por %xmm4,%xmm2; por DES_bs_all_tmp+(3)*16,%xmm7; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(8)*16,%xmm1; por %xmm5,%xmm6; movdqa %xmm3,DES_bs_all_B+(27)*16; pand %xmm0,%xmm4; movdqa DES_bs_all_tmp+(8)*16,%xmm3; por %xmm0,%xmm5; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm6,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pand %xmm3,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm2,%xmm5; pxor DES_bs_all_tmp+(7)*16,%xmm0; pand %xmm4,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm4; pxor %xmm5,%xmm7; por DES_bs_all_tmp+(4)*16,%xmm7; movdqa %xmm1,%xmm5; por DES_bs_all_tmp+(3)*16,%xmm5; por %xmm2,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm2; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pand %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm4; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm6,%xmm4; pxor DES_bs_all_B+(1)*16,%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor DES_bs_all_B+(12)*16,%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor DES_bs_all_B+(17)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(12)*16; movdqa %xmm7,DES_bs_all_B+(1)*16; movdqa %xmm2,DES_bs_all_B+(17)*16
 movl (60)*4(%edx),%ecx; movl (61)*4(%edx),%esi; movdqa DES_bs_all_B+(39)*16,%xmm0; movdqa DES_bs_all_B+(40)*16,%xmm1; pxor (%ecx),%xmm0; movl (62)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (63)*4(%edx),%esi; movdqa DES_bs_all_B+(41)*16,%xmm2; movdqa DES_bs_all_B+(42)*16,%xmm3; pxor (%ecx),%xmm2; movl (64)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(43)*16,%xmm4; movl (65)*4(%edx),%esi; movdqa DES_bs_all_B+(44)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(2)*16; pand %xmm2,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(3)*16; pxor %xmm5,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pandn %xmm3,%xmm4; movdqa %xmm0,DES_bs_all_tmp+(5)*16; por %xmm3,%xmm7; movdqa DES_bs_all_tmp+(4)*16,%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pandn %xmm2,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(7)*16; pxor %xmm6,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,DES_bs_all_tmp+(8)*16; movdqa %xmm7,%xmm4; por DES_bs_all_tmp+(5)*16,%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(4)*16,%xmm7; por %xmm1,%xmm6; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pand %xmm5,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm0,%xmm7; movdqa %xmm4,DES_bs_all_tmp+(12)*16; movdqa %xmm2,%xmm4; pxor DES_bs_all_tmp+(3)*16,%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(13)*16; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(14)*16; por %xmm5,%xmm3; movdqa %xmm2,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm2,%xmm4; por DES_bs_all_tmp+(8)*16,%xmm2; pand %xmm1,%xmm7; por DES_bs_all_tmp+(5)*16,%xmm4; por %xmm1,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm7; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(1)*16,%xmm3; pxor DES_bs_all_tmp+(14)*16,%xmm4; pand %xmm3,%xmm7; pxor DES_bs_all_tmp+(13)*16,%xmm7; por %xmm3,%xmm2; movdqa %xmm4,DES_bs_all_tmp+(5)*16; pxor %xmm6,%xmm2; pxor DES_bs_all_B+(5)*16,%xmm7; por %xmm1,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(3)*16,%xmm6; por DES_bs_all_tmp+(8)*16,%xmm6; pxor DES_bs_all_tmp+(5)*16,%xmm3; pxor %xmm6,%xmm4; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pand DES_bs_all_tmp+(9)*16,%xmm6; movdqa %xmm7,DES_bs_all_B+(5)*16; movdqa DES_bs_all_tmp+(2)*16,%xmm0; pxor %xmm6,%xmm3; por DES_bs_all_tmp+(7)*16,%xmm6; pand %xmm1,%xmm3; por DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm6,%xmm3; pxor DES_bs_all_tmp+(11)*16,%xmm0; movdqa %xmm5,%xmm6; por DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm5,%xmm0; pand DES_bs_all_tmp+(12)*16,%xmm6; pxor %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor DES_bs_all_B+(29)*16,%xmm3; pxor %xmm0,%xmm6; pxor DES_bs_all_B+(23)*16,%xmm2; movdqa %xmm3,DES_bs_all_B+(29)*16; pxor DES_bs_all_B+(15)*16,%xmm6; movdqa %xmm2,DES_bs_all_B+(23)*16; movdqa %xmm6,DES_bs_all_B+(15)*16
 movl (66)*4(%edx),%ecx; movl (67)*4(%edx),%esi; movdqa DES_bs_all_B+(43)*16,%xmm0; movdqa DES_bs_all_B+(44)*16,%xmm1; pxor (%ecx),%xmm0; movl (68)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (69)*4(%edx),%esi; movdqa DES_bs_all_B+(45)*16,%xmm2; movdqa DES_bs_all_B+(46)*16,%xmm3; pxor (%ecx),%xmm2; movl (70)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(47)*16,%xmm4; movl (71)*4(%edx),%esi; movdqa DES_bs_all_B+(48)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; movdqa %xmm0,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(1)*16; por %xmm0,%xmm6; pxor (%esi),%xmm5; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,DES_bs_all_tmp+(4)*16; movdqa %xmm2,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm2; pxor %xmm5,%xmm0; pxor DES_bs_all_tmp+(0)*16,%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por DES_bs_all_tmp+(3)*16,%xmm6; pxor %xmm3,%xmm1; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm5,%xmm6; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm0,%xmm4; pxor DES_bs_all_tmp+(2)*16,%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa DES_bs_all_tmp+(3)*16,%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor DES_bs_all_B+(25)*16,%xmm1; por %xmm4,%xmm2; pand DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor DES_bs_all_B+(9)*16,%xmm6; pxor %xmm4,%xmm2; pxor DES_bs_all_B+(19)*16,%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,DES_bs_all_B+(25)*16; pxor %xmm5,%xmm6; movdqa %xmm0,DES_bs_all_B+(19)*16; pxor DES_bs_all_B+(0)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(9)*16; movdqa %xmm5,DES_bs_all_B+(0)*16
 movl (72)*4(%edx),%ecx; movl (73)*4(%edx),%esi; movdqa DES_bs_all_B+(47)*16,%xmm0; movdqa DES_bs_all_B+(48)*16,%xmm1; pxor (%ecx),%xmm0; movl (74)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (75)*4(%edx),%esi; movdqa DES_bs_all_B+(49)*16,%xmm2; movdqa DES_bs_all_B+(50)*16,%xmm3; pxor (%ecx),%xmm2; movl (76)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(51)*16,%xmm4; movl (77)*4(%edx),%esi; movdqa DES_bs_all_B+(52)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor %xmm0,%xmm1; pxor (%esi),%xmm5; pxor %xmm3,%xmm0; movdqa %xmm1,DES_bs_all_tmp+(4)*16; movdqa %xmm5,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(8)*16; pxor %xmm5,%xmm1; movdqa %xmm5,DES_bs_all_tmp+(5)*16; pand %xmm2,%xmm6; movdqa DES_bs_all_tmp+(3)*16,%xmm5; pxor %xmm3,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(7)*16; movdqa %xmm7,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(9)*16; pxor %xmm2,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pxor %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(10)*16; pandn %xmm6,%xmm7; por DES_bs_all_tmp+(3)*16,%xmm0; por %xmm4,%xmm5; movdqa %xmm6,DES_bs_all_tmp+(11)*16; pxor %xmm1,%xmm5; movdqa %xmm0,DES_bs_all_tmp+(12)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(2)*16,%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(3)*16; por %xmm7,%xmm1; pand DES_bs_all_tmp+(6)*16,%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn DES_bs_all_tmp+(8)*16,%xmm6; pxor %xmm0,%xmm5; pxor DES_bs_all_tmp+(10)*16,%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(8)*16; movdqa %xmm6,%xmm5; pandn DES_bs_all_tmp+(9)*16,%xmm0; pandn %xmm1,%xmm5; pxor DES_bs_all_B+(24)*16,%xmm6; pxor %xmm2,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm2; movdqa %xmm0,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,DES_bs_all_tmp+(10)*16; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; movdqa %xmm2,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm7; pand %xmm3,%xmm1; pxor DES_bs_all_tmp+(3)*16,%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,DES_bs_all_tmp+(9)*16; pxor %xmm7,%xmm1; movdqa DES_bs_all_B+(13)*16,%xmm7; por %xmm2,%xmm3; movdqa DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pandn %xmm3,%xmm2; movdqa DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm2,%xmm7; movdqa DES_bs_all_tmp+(12)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(11)*16,%xmm2; por %xmm5,%xmm1; pxor DES_bs_all_B+(7)*16,%xmm5; pxor %xmm3,%xmm2; por DES_bs_all_tmp+(2)*16,%xmm2; movdqa %xmm7,DES_bs_all_B+(13)*16; pxor DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa DES_bs_all_tmp+(10)*16,%xmm2; pxor DES_bs_all_tmp+(9)*16,%xmm1; movdqa %xmm2,%xmm3; pxor DES_bs_all_tmp+(8)*16,%xmm2; pxor %xmm1,%xmm5; pand DES_bs_all_tmp+(7)*16,%xmm3; pandn %xmm4,%xmm2; pand DES_bs_all_tmp+(3)*16,%xmm2; pxor %xmm0,%xmm3; pxor DES_bs_all_tmp+(0)*16,%xmm6; pxor %xmm2,%xmm3; movdqa DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm6,DES_bs_all_B+(24)*16; pxor DES_bs_all_B+(2)*16,%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,DES_bs_all_B+(2)*16; movdqa %xmm5,DES_bs_all_B+(7)*16
 movl (78)*4(%edx),%ecx; movl (79)*4(%edx),%esi; movdqa DES_bs_all_B+(51)*16,%xmm0; movdqa DES_bs_all_B+(52)*16,%xmm1; pxor (%ecx),%xmm0; movl (80)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (81)*4(%edx),%esi; movdqa DES_bs_all_B+(53)*16,%xmm2; movdqa DES_bs_all_B+(54)*16,%xmm3; pxor (%ecx),%xmm2; movl (82)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(55)*16,%xmm4; movl (83)*4(%edx),%esi; movdqa DES_bs_all_B+(56)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm2,DES_bs_all_tmp+(3)*16; pxor (%esi),%xmm5; movdqa %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm4,%xmm2; movdqa %xmm3,DES_bs_all_tmp+(4)*16; pxor %xmm1,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,DES_bs_all_tmp+(6)*16; pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(5)*16; movdqa %xmm5,%xmm3; pand DES_bs_all_tmp+(2)*16,%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,DES_bs_all_tmp+(1)*16; por %xmm2,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(8)*16; pand %xmm6,%xmm0; movdqa %xmm3,DES_bs_all_tmp+(10)*16; pxor %xmm0,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,DES_bs_all_tmp+(9)*16; pand %xmm1,%xmm0; movdqa %xmm7,DES_bs_all_tmp+(7)*16; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(6)*16,%xmm6; pxor %xmm7,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,DES_bs_all_tmp+(11)*16; pand %xmm7,%xmm2; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm2,%xmm6; pxor DES_bs_all_tmp+(2)*16,%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand DES_bs_all_tmp+(3)*16,%xmm1; pand %xmm4,%xmm6; movdqa DES_bs_all_tmp+(8)*16,%xmm3; pxor %xmm1,%xmm0; pxor DES_bs_all_B+(28)*16,%xmm0; por %xmm2,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm7,%xmm4; movdqa DES_bs_all_tmp+(7)*16,%xmm1; pxor %xmm3,%xmm4; pxor DES_bs_all_tmp+(0)*16,%xmm2; por %xmm4,%xmm5; movdqa %xmm0,DES_bs_all_B+(28)*16; movdqa %xmm5,%xmm3; pandn DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm6,%xmm1; movdqa DES_bs_all_tmp+(8)*16,%xmm0; pxor %xmm2,%xmm3; por DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; pand DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm0; por DES_bs_all_tmp+(7)*16,%xmm6; movdqa %xmm7,%xmm1; pxor DES_bs_all_tmp+(10)*16,%xmm7; pxor %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm3; pxor %xmm5,%xmm7; pand DES_bs_all_tmp+(9)*16,%xmm5; por %xmm3,%xmm7; pxor DES_bs_all_tmp+(8)*16,%xmm6; por %xmm3,%xmm5; por DES_bs_all_tmp+(11)*16,%xmm1; pxor %xmm6,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm1; movdqa DES_bs_all_tmp+(3)*16,%xmm3; pxor %xmm1,%xmm7; pxor DES_bs_all_B+(18)*16,%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor DES_bs_all_B+(3)*16,%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,DES_bs_all_B+(18)*16; pxor DES_bs_all_B+(10)*16,%xmm2; movdqa %xmm0,DES_bs_all_B+(3)*16; movdqa %xmm2,DES_bs_all_B+(10)*16
 movl (84)*4(%edx),%ecx; movl (85)*4(%edx),%esi; movdqa DES_bs_all_B+(55)*16,%xmm0; movdqa DES_bs_all_B+(56)*16,%xmm1; pxor (%ecx),%xmm0; movl (86)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (87)*4(%edx),%esi; movdqa DES_bs_all_B+(57)*16,%xmm2; movdqa DES_bs_all_B+(58)*16,%xmm3; pxor (%ecx),%xmm2; movl (88)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(59)*16,%xmm4; movl (89)*4(%edx),%esi; movdqa DES_bs_all_B+(60)*16,%xmm5; pxor (%ecx),%xmm4
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; movdqa %xmm1,%xmm6; pxor (%esi),%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm3,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(4)*16; pand %xmm3,%xmm6; movdqa %xmm3,DES_bs_all_tmp+(3)*16; pxor %xmm4,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(5)*16; por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,DES_bs_all_tmp+(6)*16; pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,DES_bs_all_tmp+(9)*16; pxor %xmm5,%xmm0; por DES_bs_all_tmp+(4)*16,%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(10)*16; pxor %xmm6,%xmm0; movdqa %xmm5,DES_bs_all_tmp+(7)*16; movdqa %xmm3,%xmm4; movdqa DES_bs_all_tmp+(4)*16,%xmm6; movdqa %xmm0,%xmm5; pxor DES_bs_all_tmp+(5)*16,%xmm5; por %xmm6,%xmm4; movdqa %xmm7,DES_bs_all_tmp+(11)*16; por %xmm6,%xmm5; movdqa DES_bs_all_tmp+(1)*16,%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,DES_bs_all_tmp+(12)*16; pand %xmm5,%xmm7; movdqa %xmm0,DES_bs_all_tmp+(8)*16; pxor %xmm0,%xmm7; movdqa DES_bs_all_tmp+(3)*16,%xmm3; movdqa %xmm7,%xmm0; por DES_bs_all_tmp+(2)*16,%xmm0; pand %xmm3,%xmm1; pand DES_bs_all_tmp+(9)*16,%xmm3; por DES_bs_all_tmp+(6)*16,%xmm2; pxor DES_bs_all_tmp+(5)*16,%xmm0; pxor %xmm3,%xmm2; movdqa DES_bs_all_tmp+(2)*16,%xmm3; movdqa %xmm0,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor DES_bs_all_tmp+(10)*16,%xmm6; por %xmm3,%xmm1; pand DES_bs_all_tmp+(12)*16,%xmm0; pxor %xmm6,%xmm4; pand DES_bs_all_tmp+(4)*16,%xmm0; por %xmm3,%xmm6; por DES_bs_all_tmp+(4)*16,%xmm6; pand %xmm5,%xmm3; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm0; por DES_bs_all_tmp+(1)*16,%xmm0; pxor %xmm6,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm1; pxor %xmm4,%xmm0; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pxor %xmm2,%xmm5; movdqa DES_bs_all_tmp+(3)*16,%xmm6; por %xmm2,%xmm4; pxor DES_bs_all_tmp+(11)*16,%xmm6; pxor %xmm4,%xmm1; movdqa DES_bs_all_tmp+(4)*16,%xmm4; pand %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm3; pand %xmm4,%xmm6; pxor DES_bs_all_tmp+(10)*16,%xmm3; pxor %xmm5,%xmm6; pxor DES_bs_all_tmp+(7)*16,%xmm2; por %xmm4,%xmm3; por DES_bs_all_tmp+(1)*16,%xmm6; pxor %xmm2,%xmm3; pxor DES_bs_all_B+(31)*16,%xmm7; pxor %xmm6,%xmm3; pxor DES_bs_all_B+(11)*16,%xmm1; movdqa %xmm7,DES_bs_all_B+(31)*16; pxor DES_bs_all_B+(21)*16,%xmm3; movdqa %xmm1,DES_bs_all_B+(11)*16; pxor DES_bs_all_B+(6)*16,%xmm0; movdqa %xmm3,DES_bs_all_B+(21)*16; movdqa %xmm0,DES_bs_all_B+(6)*16
 movl (90)*4(%edx),%ecx; movl (91)*4(%edx),%esi; movdqa DES_bs_all_B+(59)*16,%xmm0; movdqa DES_bs_all_B+(60)*16,%xmm1; pxor (%ecx),%xmm0; movl (92)*4(%edx),%ecx; pxor (%esi),%xmm1; movl (93)*4(%edx),%esi; movdqa DES_bs_all_B+(61)*16,%xmm2; movdqa DES_bs_all_B+(62)*16,%xmm3; pxor (%ecx),%xmm2; movl (94)*4(%edx),%ecx; pxor (%esi),%xmm3; movdqa DES_bs_all_B+(63)*16,%xmm4; movl (95)*4(%edx),%esi; movdqa DES_bs_all_B+(32)*16,%xmm5; pxor (%ecx),%xmm4
 addl $96*4,%edx
 movdqa %xmm0,DES_bs_all_tmp+(1)*16; pxor (%esi),%xmm5; movdqa %xmm2,%xmm6; pxor DES_bs_all_tmp+(0)*16,%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,DES_bs_all_tmp+(3)*16; por %xmm0,%xmm7; pxor DES_bs_all_tmp+(0)*16,%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,DES_bs_all_tmp+(5)*16; movdqa %xmm4,%xmm5; movdqa %xmm1,DES_bs_all_tmp+(2)*16; movdqa %xmm7,%xmm1; movdqa %xmm4,DES_bs_all_tmp+(4)*16; pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,DES_bs_all_tmp+(7)*16; pand %xmm3,%xmm5; por DES_bs_all_tmp+(2)*16,%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor DES_bs_all_tmp+(1)*16,%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,DES_bs_all_tmp+(6)*16; pand %xmm4,%xmm5; movdqa %xmm7,DES_bs_all_tmp+(8)*16; movdqa %xmm0,%xmm1; pand DES_bs_all_tmp+(4)*16,%xmm3; movdqa %xmm0,%xmm7; pand DES_bs_all_tmp+(4)*16,%xmm1; pxor %xmm3,%xmm7; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm1,%xmm6; movdqa DES_bs_all_tmp+(5)*16,%xmm1; pxor %xmm4,%xmm7; por DES_bs_all_tmp+(2)*16,%xmm6; pandn %xmm0,%xmm4; pxor DES_bs_all_tmp+(6)*16,%xmm6; pand %xmm2,%xmm1; pxor DES_bs_all_tmp+(1)*16,%xmm3; pxor %xmm6,%xmm2; por DES_bs_all_tmp+(5)*16,%xmm6; pxor %xmm7,%xmm1; pxor DES_bs_all_tmp+(7)*16,%xmm3; pxor %xmm7,%xmm6; por DES_bs_all_tmp+(2)*16,%xmm4; pand DES_bs_all_tmp+(2)*16,%xmm5; pxor %xmm4,%xmm3; movdqa DES_bs_all_tmp+(1)*16,%xmm4; pand DES_bs_all_tmp+(8)*16,%xmm4; por DES_bs_all_tmp+(3)*16,%xmm7; pxor %xmm4,%xmm0; pand DES_bs_all_tmp+(2)*16,%xmm7; pxor %xmm0,%xmm5; movdqa DES_bs_all_tmp+(5)*16,%xmm4; por %xmm0,%xmm2; pxor DES_bs_all_tmp+(8)*16,%xmm7; por %xmm4,%xmm5; pxor DES_bs_all_B+(4)*16,%xmm6; pand %xmm4,%xmm2; pxor DES_bs_all_B+(20)*16,%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor DES_bs_all_B+(14)*16,%xmm5; movdqa %xmm6,DES_bs_all_B+(4)*16; pxor DES_bs_all_B+(26)*16,%xmm2; movdqa %xmm1,DES_bs_all_B+(20)*16; movdqa %xmm5,DES_bs_all_B+(14)*16; movdqa %xmm2,DES_bs_all_B+(26)*16
 decl %eax
 jnz DES_bs_crypt_LM_loop
 popl %esi
 ret
# 119 "crypt64.S" 2
.endif
# 200 "crypt64.S"
.text
.align 16
.globl crypt64_desc
crypt64_desc:
.globl _crypt64_desc
_crypt64_desc:
 .long crypt64_pro
 .long crypt64_crypt
 .long crypt64_cmp_pro
 .long crypt64_cmp_ep
 .long crypt64_ep
 .long crypt64_ep_end
 .byte 16/16
 .byte 4
 .word (loe-los)/(2*48)



los:
 .word (Ln00-crypt64_crypt-4),(Ln10-crypt64_crypt-4),(Ln20-crypt64_crypt-4),(Ln30-crypt64_crypt-4),(Ln40-crypt64_crypt-4),(Ln50-crypt64_crypt-4)
 .word (Ln01-crypt64_crypt-4),(Ln11-crypt64_crypt-4),(Ln21-crypt64_crypt-4),(Ln31-crypt64_crypt-4),(Ln41-crypt64_crypt-4),(Ln51-crypt64_crypt-4)
 .word (Ln02-crypt64_crypt-4),(Ln12-crypt64_crypt-4),(Ln22-crypt64_crypt-4),(Ln32-crypt64_crypt-4),(Ln42-crypt64_crypt-4),(Ln52-crypt64_crypt-4)
 .word (Ln03-crypt64_crypt-4),(Ln13-crypt64_crypt-4),(Ln23-crypt64_crypt-4),(Ln33-crypt64_crypt-4),(Ln43-crypt64_crypt-4),(Ln53-crypt64_crypt-4)
 .word (Ln04-crypt64_crypt-4),(Ln14-crypt64_crypt-4),(Ln24-crypt64_crypt-4),(Ln34-crypt64_crypt-4),(Ln44-crypt64_crypt-4),(Ln54-crypt64_crypt-4)
 .word (Ln05-crypt64_crypt-4),(Ln15-crypt64_crypt-4),(Ln25-crypt64_crypt-4),(Ln35-crypt64_crypt-4),(Ln45-crypt64_crypt-4),(Ln55-crypt64_crypt-4)
 .word (Ln06-crypt64_crypt-4),(Ln16-crypt64_crypt-4),(Ln26-crypt64_crypt-4),(Ln36-crypt64_crypt-4),(Ln46-crypt64_crypt-4),(Ln56-crypt64_crypt-4)
 .word (Ln07-crypt64_crypt-4),(Ln17-crypt64_crypt-4),(Ln27-crypt64_crypt-4),(Ln37-crypt64_crypt-4),(Ln47-crypt64_crypt-4),(Ln57-crypt64_crypt-4)
loe:
.align 64

.globl _crypt64
_crypt64:
crypt64_pro:
 push %ebp
 push %edi
 push %ebx
 push %esi
# 249 "crypt64.S"
 push %eax

crypt64_crypt:



 pxor %xmm7,%xmm7
 mov $-16*64,%eax
clr_lr:
 movdqa %xmm7,16*64(%edx,%eax)
 add $16,%eax
 jne clr_lr


 lea 16*(16)(%edx),%ebp
 lea 16*(32+16)(%edx),%edi

 lea 16*(64+16)(%edx),%esi


 movb $0,%dl
 movw $16*25,%ax

.align 64
loo:
 add 16*2*56(%ecx),%ecx
# 318 "crypt64.S"
movdqa 16*(((31&(4*(0)-1)))-16)+0x100000(%ebp),%xmm0;Ln00:; pxor 16*13(%ecx),%xmm0; movdqa 16*(((31&(4*(0)+0)))-16)+0x100000(%ebp),%xmm1;Ln10:; pxor 16*16(%ecx),%xmm1; movdqa 16*(((31&(4*(0)+1)))-16)+0x100000(%ebp),%xmm2;Ln20:; pxor 16*10(%ecx),%xmm2; movdqa 16*(((31&(4*(0)+2)))-16)+0x100000(%ebp),%xmm3;Ln30:; pxor 16*23(%ecx),%xmm3; movdqa 16*(((31&(4*(0)+3)))-16)+0x100000(%ebp),%xmm4;Ln40:; pxor 16*0(%ecx),%xmm4; movdqa 16*(((31&(4*(0)+4)))-16)+0x100000(%ebp),%xmm5;Ln50:; pxor 16*4(%ecx),%xmm5;movdqa %xmm0,16*(1)(%esi); movdqa %xmm3,%xmm6; pxor (%esi),%xmm0; pxor %xmm2,%xmm3; pxor (%esi),%xmm6; movdqa %xmm0,%xmm7; ; movdqa %xmm4,16*(3)(%esi); por %xmm2,%xmm7; movdqa %xmm3,16*(5)(%esi); movdqa %xmm5,%xmm4; movdqa %xmm6,16*(4)(%esi); pxor %xmm0,%xmm3; movdqa %xmm7,16*(7)(%esi); por %xmm6,%xmm0; movdqa %xmm2,16*(2)(%esi); pand %xmm6,%xmm7; movdqa %xmm3,16*(6)(%esi); por %xmm3,%xmm2; pxor (%esi),%xmm2; pand %xmm0,%xmm4; movdqa %xmm7,%xmm6; por %xmm5,%xmm2; movdqa %xmm7,16*(8)(%esi); por %xmm5,%xmm6; pxor %xmm2,%xmm7; pxor %xmm6,%xmm3; movdqa %xmm2,16*(11)(%esi); pxor %xmm4,%xmm6; pand 16*(2)(%esi),%xmm4; movdqa %xmm6,%xmm2; pxor 16*(2)(%esi),%xmm6; por %xmm1,%xmm2; pand 16*(7)(%esi),%xmm6; pxor %xmm3,%xmm2; movdqa %xmm4,16*(13)(%esi); pxor %xmm2,%xmm0; movdqa %xmm7,16*(12)(%esi); movdqa %xmm5,%xmm4; movdqa %xmm2,16*(9)(%esi); por %xmm0,%xmm4; movdqa 16*(4)(%esi),%xmm7; por %xmm1,%xmm6; movdqa %xmm0,16*(10)(%esi); movdqa %xmm3,%xmm2; pandn 16*(5)(%esi),%xmm0; pxor %xmm7,%xmm4; por 16*(6)(%esi),%xmm5; por %xmm1,%xmm0; pxor 16*(13)(%esi),%xmm5; pxor %xmm0,%xmm4; movdqa 16*(3)(%esi),%xmm0; pand %xmm7,%xmm2; movdqa %xmm6,16*(14)(%esi); por %xmm1,%xmm2; movdqa 16*(10)(%esi),%xmm6; por %xmm4,%xmm0; pand 16*(7)(%esi),%xmm6; por %xmm3,%xmm7; movdqa %xmm5,16*(15)(%esi); pxor %xmm3,%xmm6; pxor 16*(8)(%esi),%xmm7; movdqa %xmm1,%xmm5; pxor 16*(12)(%esi),%xmm2; pand %xmm6,%xmm5; pand 16*(2)(%esi),%xmm6; pxor %xmm7,%xmm5; por 16*(3)(%esi),%xmm5; movdqa 16*(1)(%esi),%xmm7; pxor %xmm2,%xmm5; movdqa 16*(6)(%esi),%xmm2; por %xmm3,%xmm7; por 16*(13)(%esi),%xmm2; pxor %xmm6,%xmm3; pxor 16*(11)(%esi),%xmm6; pxor %xmm4,%xmm7; movdqa 16*(2)(%esi),%xmm4; por %xmm1,%xmm7; por 16*(12)(%esi),%xmm4; por %xmm1,%xmm6; pxor 16*(10)(%esi),%xmm4; pxor %xmm2,%xmm6; movdqa 16*(9)(%esi),%xmm2; pxor %xmm4,%xmm7; pxor 16*(14)(%esi),%xmm3; pxor %xmm2,%xmm0; pxor 16*(8 -16)(%edi),%xmm5; pand %xmm3,%xmm2; movdqa 16*(3)(%esi),%xmm4; pand %xmm1,%xmm2; movdqa %xmm5,16*(8 -16)(%edi); pxor 16*(15)(%esi),%xmm2; pand %xmm4,%xmm7; pxor 16*(30 -16)(%edi),%xmm0; pand %xmm4,%xmm2; pxor 16*(16 -16)(%edi),%xmm7; movdqa %xmm0,16*(30 -16)(%edi); pxor 16*(22 -16)(%edi),%xmm2; pxor %xmm6,%xmm7; pxor %xmm3,%xmm2; movdqa %xmm7,16*(16 -16)(%edi); movdqa %xmm2,16*(22 -16)(%edi)
movdqa 16*(((31&(4*(1)-1)))-16)+0x100000(%ebp),%xmm0;Ln01:; pxor 16*2(%ecx),%xmm0; movdqa 16*(((31&(4*(1)+0)))-16)+0x100000(%ebp),%xmm1;Ln11:; pxor 16*27(%ecx),%xmm1; movdqa 16*(((31&(4*(1)+1)))-16)+0x100000(%ebp),%xmm2;Ln21:; pxor 16*14(%ecx),%xmm2; movdqa 16*(((31&(4*(1)+2)))-16)+0x100000(%ebp),%xmm3;Ln31:; pxor 16*5(%ecx),%xmm3; movdqa 16*(((31&(4*(1)+3)))-16)+0x100000(%ebp),%xmm4;Ln41:; pxor 16*20(%ecx),%xmm4; movdqa 16*(((31&(4*(1)+4)))-16)+0x100000(%ebp),%xmm5;Ln51:; pxor 16*9(%ecx),%xmm5;movdqa %xmm3,16*(4)(%esi); movdqa %xmm4,%xmm6; ; movdqa %xmm0,16*(1)(%esi); movdqa %xmm4,%xmm7; pxor (%esi),%xmm0; pxor %xmm5,%xmm6; pxor (%esi),%xmm7; movdqa %xmm0,%xmm3; movdqa %xmm2,16*(3)(%esi); por %xmm5,%xmm7; movdqa %xmm6,16*(5)(%esi); por %xmm7,%xmm3; pxor %xmm4,%xmm7; pxor %xmm0,%xmm6; pand %xmm1,%xmm3; por %xmm7,%xmm2; movdqa %xmm1,16*(2)(%esi); pxor %xmm5,%xmm3; movdqa %xmm6,16*(6)(%esi); pxor %xmm1,%xmm6; movdqa %xmm7,16*(8)(%esi); pand %xmm3,%xmm1; pand 16*(3)(%esi),%xmm3; pxor %xmm2,%xmm1; movdqa 16*(6)(%esi),%xmm7; movdqa %xmm1,%xmm2; pand 16*(4)(%esi),%xmm2; pxor %xmm6,%xmm3; movdqa %xmm6,16*(7)(%esi); pxor %xmm2,%xmm3; movdqa 16*(1)(%esi),%xmm2; por %xmm5,%xmm7; por %xmm2,%xmm1; pand %xmm3,%xmm7; pxor 16*(27 -16)(%edi),%xmm3; por %xmm4,%xmm2; por 16*(3)(%esi),%xmm7; movdqa %xmm2,%xmm6; pxor 16*(8)(%esi),%xmm1; por %xmm5,%xmm6; movdqa %xmm3,16*(27 -16)(%edi); pand %xmm0,%xmm4; movdqa 16*(8)(%esi),%xmm3; por %xmm0,%xmm5; movdqa %xmm2,16*(9)(%esi); pxor %xmm6,%xmm3; movdqa 16*(2)(%esi),%xmm2; pxor %xmm6,%xmm0; pxor %xmm2,%xmm3; pand %xmm2,%xmm0; pxor %xmm3,%xmm7; por %xmm4,%xmm2; pxor 16*(5)(%esi),%xmm4; pand %xmm3,%xmm6; pxor %xmm0,%xmm4; pxor %xmm5,%xmm6; movdqa %xmm7,16*(10)(%esi); pand %xmm3,%xmm0; movdqa 16*(3)(%esi),%xmm7; pxor %xmm2,%xmm5; pxor 16*(7)(%esi),%xmm0; pand %xmm4,%xmm7; pand 16*(2)(%esi),%xmm4; pxor %xmm5,%xmm7; por 16*(4)(%esi),%xmm7; movdqa %xmm1,%xmm5; por 16*(3)(%esi),%xmm5; por %xmm2,%xmm1; pand 16*(9)(%esi),%xmm2; pxor %xmm3,%xmm4; movdqa 16*(4)(%esi),%xmm3; pand %xmm4,%xmm2; pand 16*(3)(%esi),%xmm4; pxor %xmm5,%xmm0; pxor 16*(10)(%esi),%xmm7; pxor %xmm6,%xmm4; pxor 16*(1 -16)(%edi),%xmm7; pand %xmm3,%xmm1; por %xmm3,%xmm2; pxor 16*(12 -16)(%edi),%xmm1; pxor %xmm4,%xmm2; pxor %xmm0,%xmm1; pxor 16*(17 -16)(%edi),%xmm2; movdqa %xmm1,16*(12 -16)(%edi); movdqa %xmm7,16*(1 -16)(%edi); movdqa %xmm2,16*(17 -16)(%edi)
movdqa 16*(((31&(4*(2)-1)))-16)+0x100000(%ebp),%xmm0;Ln02:; pxor 16*22(%ecx),%xmm0; movdqa 16*(((31&(4*(2)+0)))-16)+0x100000(%ebp),%xmm1;Ln12:; pxor 16*18(%ecx),%xmm1; movdqa 16*(((31&(4*(2)+1)))-16)+0x100000(%ebp),%xmm2;Ln22:; pxor 16*11(%ecx),%xmm2; movdqa 16*(((31&(4*(2)+2)))-16)+0x100000(%ebp),%xmm3;Ln32:; pxor 16*3(%ecx),%xmm3; movdqa 16*(((31&(4*(2)+3)))-16)+0x100000(%ebp),%xmm4;Ln42:; pxor 16*25(%ecx),%xmm4; movdqa 16*(((31&(4*(2)+4)))-16)+0x100000(%ebp),%xmm5;Ln52:; pxor 16*7(%ecx),%xmm5;movdqa %xmm0,16*(1)(%esi); ; movdqa %xmm4,%xmm0; movdqa %xmm5,%xmm6; pxor (%esi),%xmm6; movdqa %xmm4,%xmm7; pxor %xmm6,%xmm7; movdqa %xmm6,16*(2)(%esi); pand %xmm2,%xmm0; movdqa %xmm7,16*(3)(%esi); pxor %xmm5,%xmm0; movdqa %xmm4,16*(4)(%esi); pandn %xmm3,%xmm4; movdqa %xmm0,16*(5)(%esi); por %xmm3,%xmm7; movdqa 16*(4)(%esi),%xmm6; pxor %xmm4,%xmm0; movdqa %xmm5,16*(6)(%esi); pandn %xmm2,%xmm6; movdqa %xmm0,16*(7)(%esi); pxor %xmm6,%xmm7; movdqa 16*(2)(%esi),%xmm5; pxor %xmm1,%xmm0; movdqa %xmm4,16*(8)(%esi); movdqa %xmm7,%xmm4; por 16*(5)(%esi),%xmm5; pand %xmm0,%xmm4; movdqa %xmm7,16*(9)(%esi); pxor %xmm5,%xmm6; pxor 16*(4)(%esi),%xmm7; por %xmm1,%xmm6; movdqa %xmm4,16*(10)(%esi); pand %xmm5,%xmm4; movdqa %xmm7,16*(11)(%esi); por %xmm0,%xmm7; movdqa %xmm4,16*(12)(%esi); movdqa %xmm2,%xmm4; pxor 16*(3)(%esi),%xmm4; pand %xmm3,%xmm7; movdqa %xmm0,16*(13)(%esi); pxor %xmm3,%xmm4; pxor 16*(6)(%esi),%xmm5; pxor %xmm4,%xmm6; movdqa %xmm3,16*(14)(%esi); por %xmm5,%xmm3; movdqa %xmm2,16*(4)(%esi); pxor %xmm3,%xmm5; por %xmm1,%xmm5; pxor %xmm7,%xmm2; pxor 16*(10)(%esi),%xmm7; movdqa %xmm2,%xmm4; por 16*(8)(%esi),%xmm2; pand %xmm1,%xmm7; por 16*(5)(%esi),%xmm4; por %xmm1,%xmm2; pxor 16*(9)(%esi),%xmm7; pxor %xmm3,%xmm2; movdqa 16*(1)(%esi),%xmm3; pxor 16*(14)(%esi),%xmm4; pand %xmm3,%xmm7; pxor 16*(13)(%esi),%xmm7; por %xmm3,%xmm2; movdqa %xmm4,16*(5)(%esi); pxor %xmm6,%xmm2; pxor 16*(5 -16)(%edi),%xmm7; por %xmm1,%xmm4; movdqa 16*(4)(%esi),%xmm6; movdqa %xmm2,%xmm3; pxor 16*(3)(%esi),%xmm6; por 16*(8)(%esi),%xmm6; pxor 16*(5)(%esi),%xmm3; pxor %xmm6,%xmm4; movdqa 16*(6)(%esi),%xmm6; pand 16*(9)(%esi),%xmm6; movdqa %xmm7,16*(5 -16)(%edi); movdqa 16*(2)(%esi),%xmm0; pxor %xmm6,%xmm3; por 16*(7)(%esi),%xmm6; pand %xmm1,%xmm3; por 16*(5)(%esi),%xmm0; pxor %xmm6,%xmm3; pxor 16*(11)(%esi),%xmm0; movdqa %xmm5,%xmm6; por 16*(1)(%esi),%xmm3; pxor %xmm5,%xmm0; pand 16*(12)(%esi),%xmm6; pxor %xmm4,%xmm3; por 16*(1)(%esi),%xmm6; pxor 16*(29 -16)(%edi),%xmm3; pxor %xmm0,%xmm6; pxor 16*(23 -16)(%edi),%xmm2; movdqa %xmm3,16*(29 -16)(%edi); pxor 16*(15 -16)(%edi),%xmm6; movdqa %xmm2,16*(23 -16)(%edi); movdqa %xmm6,16*(15 -16)(%edi)
movdqa 16*(((31&(4*(3)-1)))-16)+0x100000(%ebp),%xmm0;Ln03:; pxor 16*15(%ecx),%xmm0; movdqa 16*(((31&(4*(3)+0)))-16)+0x100000(%ebp),%xmm1;Ln13:; pxor 16*6(%ecx),%xmm1; movdqa 16*(((31&(4*(3)+1)))-16)+0x100000(%ebp),%xmm2;Ln23:; pxor 16*26(%ecx),%xmm2; movdqa 16*(((31&(4*(3)+2)))-16)+0x100000(%ebp),%xmm3;Ln33:; pxor 16*19(%ecx),%xmm3; movdqa 16*(((31&(4*(3)+3)))-16)+0x100000(%ebp),%xmm4;Ln43:; pxor 16*12(%ecx),%xmm4; movdqa 16*(((31&(4*(3)+4)))-16)+0x100000(%ebp),%xmm5;Ln53:; pxor 16*1(%ecx),%xmm5;movdqa %xmm2,%xmm6; movdqa %xmm3,16*(3)(%esi); movdqa %xmm0,%xmm7; movdqa %xmm1,16*(1)(%esi); por %xmm0,%xmm6; ; pand %xmm4,%xmm7; movdqa %xmm1,%xmm3; movdqa %xmm5,16*(4)(%esi); movdqa %xmm2,16*(2)(%esi); movdqa %xmm4,%xmm5; pand %xmm6,%xmm5; por %xmm2,%xmm3; pxor (%esi),%xmm2; pxor %xmm5,%xmm0; pxor (%esi),%xmm0; pxor %xmm7,%xmm6; pxor %xmm0,%xmm3; movdqa %xmm1,%xmm7; pand %xmm6,%xmm7; pxor %xmm2,%xmm5; pxor %xmm4,%xmm2; pand %xmm5,%xmm0; pxor %xmm7,%xmm4; pand %xmm1,%xmm5; por %xmm1,%xmm2; pxor %xmm6,%xmm5; movdqa 16*(3)(%esi),%xmm1; movdqa %xmm0,%xmm6; pand %xmm4,%xmm1; pxor %xmm2,%xmm6; por 16*(3)(%esi),%xmm6; pxor %xmm3,%xmm1; pand 16*(1)(%esi),%xmm4; pxor %xmm5,%xmm6; movdqa 16*(4)(%esi),%xmm3; pxor %xmm0,%xmm4; pxor 16*(2)(%esi),%xmm7; movdqa %xmm3,%xmm0; pxor %xmm2,%xmm7; pand %xmm6,%xmm0; movdqa 16*(3)(%esi),%xmm2; por %xmm3,%xmm6; pxor %xmm1,%xmm0; pand %xmm2,%xmm7; pxor (%esi),%xmm1; pxor %xmm7,%xmm4; movdqa %xmm4,%xmm5; pxor %xmm1,%xmm4; pxor 16*(25 -16)(%edi),%xmm1; por %xmm4,%xmm2; pand 16*(1)(%esi),%xmm4; pxor %xmm6,%xmm1; pxor %xmm0,%xmm4; pxor 16*(9 -16)(%edi),%xmm6; pxor %xmm4,%xmm2; pxor 16*(19 -16)(%edi),%xmm0; pand %xmm2,%xmm3; pxor %xmm2,%xmm6; pxor %xmm3,%xmm5; movdqa %xmm1,16*(25 -16)(%edi); pxor %xmm5,%xmm6; movdqa %xmm0,16*(19 -16)(%edi); pxor 16*(0 -16)(%edi),%xmm5; movdqa %xmm6,16*(9 -16)(%edi); movdqa %xmm5,16*(0 -16)(%edi)
movdqa 16*(((31&(4*(4)-1)))-16)+0x100000(%ebp),%xmm0;Ln04:; pxor 16*68(%ecx),%xmm0; movdqa 16*(((31&(4*(4)+0)))-16)+0x100000(%ebp),%xmm1;Ln14:; pxor 16*79(%ecx),%xmm1; movdqa 16*(((31&(4*(4)+1)))-16)+0x100000(%ebp),%xmm2;Ln24:; pxor 16*58(%ecx),%xmm2; movdqa 16*(((31&(4*(4)+2)))-16)+0x100000(%ebp),%xmm3;Ln34:; pxor 16*64(%ecx),%xmm3; movdqa 16*(((31&(4*(4)+3)))-16)+0x100000(%ebp),%xmm4;Ln44:; pxor 16*74(%ecx),%xmm4; movdqa 16*(((31&(4*(4)+4)))-16)+0x100000(%ebp),%xmm5;Ln54:; pxor 16*82(%ecx),%xmm5;movdqa %xmm1,16*(2)(%esi); movdqa %xmm3,%xmm6; movdqa %xmm2,%xmm7; pandn %xmm2,%xmm6; pandn %xmm0,%xmm7; movdqa %xmm6,%xmm1; movdqa %xmm0,16*(1)(%esi); pxor %xmm0,%xmm1; ; pxor %xmm3,%xmm0; movdqa %xmm1,16*(4)(%esi); movdqa %xmm5,16*(3)(%esi); por %xmm0,%xmm6; por %xmm7,%xmm5; movdqa %xmm6,16*(8)(%esi); pxor %xmm5,%xmm1; movdqa %xmm5,16*(5)(%esi); pand %xmm2,%xmm6; movdqa 16*(3)(%esi),%xmm5; pxor %xmm3,%xmm6; pandn 16*(8)(%esi),%xmm5; movdqa %xmm0,16*(7)(%esi); movdqa %xmm7,%xmm0; movdqa %xmm5,16*(9)(%esi); pxor %xmm2,%xmm5; movdqa %xmm1,16*(6)(%esi); pxor %xmm3,%xmm0; movdqa %xmm5,16*(10)(%esi); pandn %xmm6,%xmm7; por 16*(3)(%esi),%xmm0; por %xmm4,%xmm5; movdqa %xmm6,16*(11)(%esi); pxor %xmm1,%xmm5; movdqa %xmm0,16*(12)(%esi); pxor %xmm0,%xmm7; movdqa 16*(2)(%esi),%xmm0; movdqa %xmm4,%xmm1; movdqa %xmm7,16*(3)(%esi); por %xmm7,%xmm1; pand 16*(6)(%esi),%xmm7; pxor %xmm6,%xmm1; pandn %xmm1,%xmm0; movdqa %xmm7,%xmm6; pandn 16*(8)(%esi),%xmm6; pxor %xmm0,%xmm5; pxor 16*(10)(%esi),%xmm7; movdqa %xmm3,%xmm0; movdqa %xmm5,16*(8)(%esi); movdqa %xmm6,%xmm5; pandn 16*(9)(%esi),%xmm0; pandn %xmm1,%xmm5; pxor 16*(24 -16)(%edi),%xmm6; pxor %xmm2,%xmm0; movdqa 16*(1)(%esi),%xmm2; movdqa %xmm0,%xmm1; pxor 16*(10)(%esi),%xmm2; pand %xmm4,%xmm1; movdqa %xmm7,16*(10)(%esi); pxor %xmm1,%xmm6; movdqa 16*(5)(%esi),%xmm1; movdqa %xmm2,%xmm7; pand 16*(4)(%esi),%xmm7; pand %xmm3,%xmm1; pxor 16*(3)(%esi),%xmm1; pandn %xmm4,%xmm7; movdqa %xmm2,16*(9)(%esi); pxor %xmm7,%xmm1; movdqa 16*(13 -16)(%edi),%xmm7; por %xmm2,%xmm3; movdqa 16*(2)(%esi),%xmm2; pxor %xmm1,%xmm7; movdqa %xmm3,16*(3)(%esi); pandn %xmm3,%xmm2; movdqa 16*(10)(%esi),%xmm3; pxor %xmm2,%xmm7; movdqa 16*(12)(%esi),%xmm2; por %xmm4,%xmm3; por 16*(11)(%esi),%xmm2; por %xmm5,%xmm1; pxor 16*(7 -16)(%edi),%xmm5; pxor %xmm3,%xmm2; por 16*(2)(%esi),%xmm2; movdqa %xmm7,16*(13 -16)(%edi); pxor 16*(7)(%esi),%xmm1; pxor %xmm2,%xmm6; pandn %xmm4,%xmm1; movdqa 16*(10)(%esi),%xmm2; pxor 16*(9)(%esi),%xmm1; movdqa %xmm2,%xmm3; pxor 16*(8)(%esi),%xmm2; pxor %xmm1,%xmm5; pand 16*(7)(%esi),%xmm3; pandn %xmm4,%xmm2; pand 16*(3)(%esi),%xmm2; pxor %xmm0,%xmm3; pxor (%esi),%xmm6; pxor %xmm2,%xmm3; movdqa 16*(8)(%esi),%xmm4; por 16*(2)(%esi),%xmm3; movdqa %xmm6,16*(24 -16)(%edi); pxor 16*(2 -16)(%edi),%xmm4; pxor %xmm3,%xmm5; movdqa %xmm4,16*(2 -16)(%edi); movdqa %xmm5,16*(7 -16)(%edi)
movdqa 16*(((31&(4*(5)-1)))-16)+0x100000(%ebp),%xmm0;Ln05:; pxor 16*57(%ecx),%xmm0; movdqa 16*(((31&(4*(5)+0)))-16)+0x100000(%ebp),%xmm1;Ln15:; pxor 16*67(%ecx),%xmm1; movdqa 16*(((31&(4*(5)+1)))-16)+0x100000(%ebp),%xmm2;Ln25:; pxor 16*78(%ecx),%xmm2; movdqa 16*(((31&(4*(5)+2)))-16)+0x100000(%ebp),%xmm3;Ln35:; pxor 16*72(%ecx),%xmm3; movdqa 16*(((31&(4*(5)+3)))-16)+0x100000(%ebp),%xmm4;Ln45:; pxor 16*60(%ecx),%xmm4; movdqa 16*(((31&(4*(5)+4)))-16)+0x100000(%ebp),%xmm5;Ln55:; pxor 16*75(%ecx),%xmm5;movdqa %xmm2,16*(3)(%esi); ; movdqa %xmm4,%xmm6; pxor (%esi),%xmm6; movdqa %xmm5,%xmm7; movdqa %xmm1,16*(2)(%esi); movdqa %xmm4,%xmm2; movdqa %xmm3,16*(4)(%esi); pxor %xmm1,%xmm7; pxor (%esi),%xmm1; pxor %xmm6,%xmm7; movdqa %xmm6,16*(6)(%esi); pxor %xmm0,%xmm7; pand %xmm5,%xmm2; movdqa %xmm4,%xmm6; movdqa %xmm1,16*(5)(%esi); movdqa %xmm5,%xmm3; pand 16*(2)(%esi),%xmm3; pand %xmm7,%xmm6; movdqa %xmm0,16*(1)(%esi); por %xmm2,%xmm1; movdqa %xmm2,16*(8)(%esi); pand %xmm6,%xmm0; movdqa %xmm3,16*(10)(%esi); pxor %xmm0,%xmm1; movdqa 16*(4)(%esi),%xmm0; movdqa %xmm4,%xmm2; movdqa %xmm6,16*(9)(%esi); pand %xmm1,%xmm0; movdqa %xmm7,16*(7)(%esi); pxor %xmm3,%xmm2; movdqa 16*(6)(%esi),%xmm6; pxor %xmm7,%xmm0; movdqa 16*(1)(%esi),%xmm7; pxor %xmm5,%xmm1; movdqa %xmm2,16*(11)(%esi); pand %xmm7,%xmm2; movdqa 16*(4)(%esi),%xmm3; pxor %xmm2,%xmm6; pxor 16*(2)(%esi),%xmm2; pand %xmm7,%xmm1; por %xmm6,%xmm3; pxor %xmm5,%xmm6; pxor %xmm3,%xmm1; pand %xmm6,%xmm7; pand 16*(3)(%esi),%xmm1; pand %xmm4,%xmm6; movdqa 16*(8)(%esi),%xmm3; pxor %xmm1,%xmm0; pxor 16*(28 -16)(%edi),%xmm0; por %xmm2,%xmm3; pand 16*(4)(%esi),%xmm3; pxor %xmm7,%xmm4; movdqa 16*(7)(%esi),%xmm1; pxor %xmm3,%xmm4; pxor (%esi),%xmm2; por %xmm4,%xmm5; movdqa %xmm0,16*(28 -16)(%edi); movdqa %xmm5,%xmm3; pandn 16*(4)(%esi),%xmm3; pxor %xmm6,%xmm1; movdqa 16*(8)(%esi),%xmm0; pxor %xmm2,%xmm3; por 16*(4)(%esi),%xmm1; pxor %xmm3,%xmm0; pand 16*(3)(%esi),%xmm3; pxor %xmm1,%xmm0; por 16*(7)(%esi),%xmm6; movdqa %xmm7,%xmm1; pxor 16*(10)(%esi),%xmm7; pxor %xmm3,%xmm4; movdqa 16*(4)(%esi),%xmm3; pxor %xmm5,%xmm7; pand 16*(9)(%esi),%xmm5; por %xmm3,%xmm7; pxor 16*(8)(%esi),%xmm6; por %xmm3,%xmm5; por 16*(11)(%esi),%xmm1; pxor %xmm6,%xmm5; pxor 16*(5)(%esi),%xmm1; movdqa 16*(3)(%esi),%xmm3; pxor %xmm1,%xmm7; pxor 16*(18 -16)(%edi),%xmm4; por %xmm3,%xmm7; pand %xmm1,%xmm2; pxor 16*(3 -16)(%edi),%xmm0; por %xmm3,%xmm2; pxor %xmm7,%xmm0; pxor %xmm5,%xmm2; movdqa %xmm4,16*(18 -16)(%edi); pxor 16*(10 -16)(%edi),%xmm2; movdqa %xmm0,16*(3 -16)(%edi); movdqa %xmm2,16*(10 -16)(%edi)
movdqa 16*(((31&(4*(6)-1)))-16)+0x100000(%ebp),%xmm0;Ln06:; pxor 16*71(%ecx),%xmm0; movdqa 16*(((31&(4*(6)+0)))-16)+0x100000(%ebp),%xmm1;Ln16:; pxor 16*76(%ecx),%xmm1; movdqa 16*(((31&(4*(6)+1)))-16)+0x100000(%ebp),%xmm2;Ln26:; pxor 16*66(%ecx),%xmm2; movdqa 16*(((31&(4*(6)+2)))-16)+0x100000(%ebp),%xmm3;Ln36:; pxor 16*83(%ecx),%xmm3; movdqa 16*(((31&(4*(6)+3)))-16)+0x100000(%ebp),%xmm4;Ln46:; pxor 16*61(%ecx),%xmm4; movdqa 16*(((31&(4*(6)+4)))-16)+0x100000(%ebp),%xmm5;Ln56:; pxor 16*80(%ecx),%xmm5;movdqa %xmm0,16*(1)(%esi); movdqa %xmm1,%xmm6; ; movdqa %xmm1,16*(2)(%esi); movdqa %xmm3,%xmm7; movdqa %xmm5,16*(4)(%esi); pand %xmm3,%xmm6; movdqa %xmm3,16*(3)(%esi); pxor %xmm4,%xmm6; pxor (%esi),%xmm4; pand %xmm6,%xmm7; pand %xmm4,%xmm3; movdqa %xmm1,%xmm5; pxor %xmm2,%xmm6; pxor %xmm7,%xmm5; movdqa %xmm7,16*(5)(%esi); por %xmm1,%xmm4; por %xmm3,%xmm1; pxor %xmm6,%xmm7; movdqa %xmm5,16*(6)(%esi); pand %xmm2,%xmm4; pand %xmm2,%xmm5; por %xmm7,%xmm3; movdqa %xmm1,16*(9)(%esi); pxor %xmm5,%xmm0; por 16*(4)(%esi),%xmm0; pxor %xmm4,%xmm1; movdqa %xmm4,16*(10)(%esi); pxor %xmm6,%xmm0; movdqa %xmm5,16*(7)(%esi); movdqa %xmm3,%xmm4; movdqa 16*(4)(%esi),%xmm6; movdqa %xmm0,%xmm5; pxor 16*(5)(%esi),%xmm5; por %xmm6,%xmm4; movdqa %xmm7,16*(11)(%esi); por %xmm6,%xmm5; movdqa 16*(1)(%esi),%xmm7; pxor %xmm1,%xmm5; movdqa %xmm3,16*(12)(%esi); pand %xmm5,%xmm7; movdqa %xmm0,16*(8)(%esi); pxor %xmm0,%xmm7; movdqa 16*(3)(%esi),%xmm3; movdqa %xmm7,%xmm0; por 16*(2)(%esi),%xmm0; pand %xmm3,%xmm1; pand 16*(9)(%esi),%xmm3; por 16*(6)(%esi),%xmm2; pxor 16*(5)(%esi),%xmm0; pxor %xmm3,%xmm2; movdqa 16*(2)(%esi),%xmm3; movdqa %xmm0,%xmm6; pxor (%esi),%xmm3; pxor 16*(10)(%esi),%xmm6; por %xmm3,%xmm1; pand 16*(12)(%esi),%xmm0; pxor %xmm6,%xmm4; pand 16*(4)(%esi),%xmm0; por %xmm3,%xmm6; por 16*(4)(%esi),%xmm6; pand %xmm5,%xmm3; pand 16*(4)(%esi),%xmm1; pxor %xmm3,%xmm0; por 16*(1)(%esi),%xmm0; pxor %xmm6,%xmm2; pxor 16*(8)(%esi),%xmm1; pxor %xmm4,%xmm0; movdqa 16*(1)(%esi),%xmm4; pxor %xmm2,%xmm5; movdqa 16*(3)(%esi),%xmm6; por %xmm2,%xmm4; pxor 16*(11)(%esi),%xmm6; pxor %xmm4,%xmm1; movdqa 16*(4)(%esi),%xmm4; pand %xmm1,%xmm6; movdqa 16*(5)(%esi),%xmm3; pand %xmm4,%xmm6; pxor 16*(10)(%esi),%xmm3; pxor %xmm5,%xmm6; pxor 16*(7)(%esi),%xmm2; por %xmm4,%xmm3; por 16*(1)(%esi),%xmm6; pxor %xmm2,%xmm3; pxor 16*(31 -16)(%edi),%xmm7; pxor %xmm6,%xmm3; pxor 16*(11 -16)(%edi),%xmm1; movdqa %xmm7,16*(31 -16)(%edi); pxor 16*(21 -16)(%edi),%xmm3; movdqa %xmm1,16*(11 -16)(%edi); pxor 16*(6 -16)(%edi),%xmm0; movdqa %xmm3,16*(21 -16)(%edi); movdqa %xmm0,16*(6 -16)(%edi)
movdqa 16*(((31&(4*(7)-1)))-16)+0x100000(%ebp),%xmm0;Ln07:; pxor 16*73(%ecx),%xmm0; movdqa 16*(((31&(4*(7)+0)))-16)+0x100000(%ebp),%xmm1;Ln17:; pxor 16*69(%ecx),%xmm1; movdqa 16*(((31&(4*(7)+1)))-16)+0x100000(%ebp),%xmm2;Ln27:; pxor 16*77(%ecx),%xmm2; movdqa 16*(((31&(4*(7)+2)))-16)+0x100000(%ebp),%xmm3;Ln37:; pxor 16*63(%ecx),%xmm3; movdqa 16*(((31&(4*(7)+3)))-16)+0x100000(%ebp),%xmm4;Ln47:; pxor 16*56(%ecx),%xmm4; movdqa 16*(((31&(4*(7)+4)))-16)+0x100000(%ebp),%xmm5;Ln57:; pxor 16*59(%ecx),%xmm5;movdqa %xmm0,16*(1)(%esi); ; movdqa %xmm2,%xmm6; pxor (%esi),%xmm0; movdqa %xmm2,%xmm7; movdqa %xmm3,16*(3)(%esi); por %xmm0,%xmm7; pxor (%esi),%xmm3; pxor %xmm0,%xmm6; movdqa %xmm5,16*(5)(%esi); movdqa %xmm4,%xmm5; movdqa %xmm1,16*(2)(%esi); movdqa %xmm7,%xmm1; movdqa %xmm4,16*(4)(%esi); pxor %xmm3,%xmm7; por %xmm6,%xmm5; por %xmm7,%xmm0; pand %xmm4,%xmm1; pandn %xmm0,%xmm2; por %xmm7,%xmm4; pxor %xmm1,%xmm2; movdqa %xmm5,16*(7)(%esi); pand %xmm3,%xmm5; por 16*(2)(%esi),%xmm2; pxor %xmm4,%xmm7; pxor %xmm0,%xmm3; movdqa %xmm4,%xmm1; pxor 16*(7)(%esi),%xmm7; pxor %xmm3,%xmm1; pxor %xmm6,%xmm4; pxor %xmm5,%xmm2; pxor 16*(1)(%esi),%xmm5; pand %xmm3,%xmm6; movdqa %xmm1,16*(6)(%esi); pand %xmm4,%xmm5; movdqa %xmm7,16*(8)(%esi); movdqa %xmm0,%xmm1; pand 16*(4)(%esi),%xmm3; movdqa %xmm0,%xmm7; pand 16*(4)(%esi),%xmm1; pxor %xmm3,%xmm7; pand 16*(2)(%esi),%xmm7; pxor %xmm1,%xmm6; movdqa 16*(5)(%esi),%xmm1; pxor %xmm4,%xmm7; por 16*(2)(%esi),%xmm6; pandn %xmm0,%xmm4; pxor 16*(6)(%esi),%xmm6; pand %xmm2,%xmm1; pxor 16*(1)(%esi),%xmm3; pxor %xmm6,%xmm2; por 16*(5)(%esi),%xmm6; pxor %xmm7,%xmm1; pxor 16*(7)(%esi),%xmm3; pxor %xmm7,%xmm6; por 16*(2)(%esi),%xmm4; pand 16*(2)(%esi),%xmm5; pxor %xmm4,%xmm3; movdqa 16*(1)(%esi),%xmm4; pand 16*(8)(%esi),%xmm4; por 16*(3)(%esi),%xmm7; pxor %xmm4,%xmm0; pand 16*(2)(%esi),%xmm7; pxor %xmm0,%xmm5; movdqa 16*(5)(%esi),%xmm4; por %xmm0,%xmm2; pxor 16*(8)(%esi),%xmm7; por %xmm4,%xmm5; pxor 16*(4 -16)(%edi),%xmm6; pand %xmm4,%xmm2; pxor 16*(20 -16)(%edi),%xmm1; pxor %xmm7,%xmm5; pxor %xmm3,%xmm2; pxor 16*(14 -16)(%edi),%xmm5; movdqa %xmm6,16*(4 -16)(%edi); pxor 16*(26 -16)(%edi),%xmm2; movdqa %xmm1,16*(20 -16)(%edi); movdqa %xmm5,16*(14 -16)(%edi); movdqa %xmm2,16*(26 -16)(%edi)
# 335 "crypt64.S"
 mov %ebp,%ebx
 addb $(256/16),%dl
 cmovne %edi,%ebp
 cmovne %ebx,%edi
 decw %ax
 jne loo


 lea 16*(32+16)(%ebp),%ecx



 lea 16*(-16)(%edi),%edx
 lea 16*(-16)(%ecx),%esi

crypt64_cmp_pro:
# 359 "crypt64.S"
 lea 16*(16)(%edx),%ebp
 lea 16*(32+16)(%edx),%edi
 lea 16*(64+16)(%edx),%esi




crypt64_cmp_ep:

crypt64_ep:

 pop %ebp
# 394 "crypt64.S"
 pop %esi //6 //T
 pop %ebx //1 //H
 pop %edi //7 //RL
 pop %ebp //5 //LR

 ret

crypt64_ep_end:
